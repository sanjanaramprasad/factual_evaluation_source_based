{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f9097a-5e22-414d-a0cf-4602b9f40e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, string\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm \n",
    "import nltk\n",
    "from experiments.RQ1.utils import get_chatgpt_response, get_atomic_facts_gpt\n",
    "from experiments.models.AlpacaModel import AlpacaInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf67b3fe-1b5f-4f64-9893-b1548267e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_assesment_instruction1 = f'''Decide if the Summary is consistent with the corresponding {{source_type}}. Note that consistency means all information in the summary is supported by the {{source_type}}.\\nAnswer \"yes\" for consistent and \"no\" for inconsistent.'''\n",
    "\n",
    "direct_assesment_instruction2 = f\"\"\"Verify if the Summary aligns with the {{source_type}} for consistency. Consistency ensures that every detail in the Summary is substantiated by the {{source_type}}.\\nAnswer \"yes\" for consistent and \"no\" for inconsistent. \"\"\"\n",
    "\n",
    "direct_assesment_instruction3 = f'''Evaluate the Summary's consistency with the {{source_type}} by confirming if all information in the summary is supported by the {{source_type}}.\\nRespond with a yes or no.'''\n",
    "\n",
    "instruction_template = {\n",
    "'Alpaca': f'### Instruction: {{instruction}}\\n\\n### Input:\\n{{source_type}}: {{source}}\\nSummary: {{summary}}\\n\\n### Response:\\n',\n",
    "'GPT': f'{{instruction}}\\n{{source_type}}: {{source}}\\nSummary: {{summary}}\\nAnswer:'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93bc95ae-5e94-41f3-8daf-65ed47497dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_atomic_facts(afact):\n",
    "    sents = nltk.sent_tokenize(afact)\n",
    "    sents = [each for each in sents if not each.strip(string.punctuation).isdigit()]\n",
    "    return '\\n'.join(sents)\n",
    "    \n",
    "class PromptBaselines():\n",
    "    def __init__(self):\n",
    "        self.instructions = {\n",
    "            \"direct_assesment_instruction1\": f'''Decide if the Summary is consistent with the corresponding {{source_type}}. Note that consistency means all information in the summary is supported by the {{source_type}}.\\nAnswer \"yes\" for consistent and \"no\" for inconsistent.''',\n",
    "            \"direct_assesment_instruction2\": f\"\"\"Verify if the Summary aligns with the {{source_type}} for consistency. Consistency ensures that every detail in the Summary is substantiated by the {{source_type}}.\\nAnswer \"yes\" for consistent and \"no\" for inconsistent. \"\"\",\n",
    "            \"direct_assesment_instruction3\": f'''Evaluate the Summary's consistency with the {{source_type}} by confirming if all information in the summary is supported by the {{source_type}}.\\nRespond with a yes or no.'''\n",
    "        }\n",
    "\n",
    "        self.prompt_templates_zeroshot = {\n",
    "            'Alpaca': f'### Instruction: {{instruction}} \\n\\n### Input:\\n{{source_type}}: {{source}}\\nSummary: {{summary}}\\n\\n### Response:\\nAnswer:',\n",
    "            'GPT': f'{{instruction}}\\n{{source_type}}: {{source}}\\nSummary: {{summary}}\\nAnswer:'\n",
    "        }\n",
    "\n",
    "\n",
    "    def get_response_zeroshot(self,\n",
    "                                     instruction_template,\n",
    "                                   source_type,\n",
    "                                   source,\n",
    "                                   summary,\n",
    "                                    model,\n",
    "                                   print_prompt = False):\n",
    "\n",
    "        model_type = 'Alpaca' if model else 'GPT'\n",
    "        instruction = instruction_template.format(source_type = source_type)\n",
    "            \n",
    "        prompt = self.prompt_templates_zeroshot[model_type].format(\n",
    "                instruction = instruction,\n",
    "                source_type = source_type,\n",
    "                source = source,\n",
    "                summary = summary\n",
    "            )\n",
    "        \n",
    "        if print_prompt:\n",
    "            print('PROMPT', prompt)\n",
    "            print('***')\n",
    "    \n",
    "\n",
    "        if not model:\n",
    "                response = get_chatgpt_response(prompt, 'gpt-4-0613')\n",
    "        else:\n",
    "                response = model.get_response(prompt, max_len = None)\n",
    "    \n",
    "        \n",
    "        return response\n",
    "\n",
    "    def direct_assessment_zeroshot(self, \n",
    "                         source, \n",
    "                         summary,\n",
    "                         source_type  = 'Dialogue',\n",
    "                         print_prompt = False,\n",
    "                         model = None):\n",
    "\n",
    "        responses = []\n",
    "        labels = []\n",
    "        \n",
    "        if source_type != 'Dialogue':\n",
    "            source = process_atomic_facts(source)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        for inst, instruction_template in self.instructions.items():\n",
    "            res = self.get_response_zeroshot(\n",
    "                instruction_template,\n",
    "                                   source_type,\n",
    "                                   source,\n",
    "                                   summary,\n",
    "                                    model,\n",
    "                                   print_prompt )\n",
    "            pred_label = 0 if 'yes' in res.lower().strip() else 1\n",
    "            \n",
    "            responses += [res]\n",
    "            labels += [pred_label]\n",
    "        return responses, labels\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "919bf28e-c23e-4f40-9162-8505f8229e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = '/home/ramprasad.sa/factual_evaluation_source_based/annotations/xformer_llm_annotated.csv'\n",
    "\n",
    "df = pd.read_csv(read_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe50872b-abce-4a85-8a95-5746c31c00f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877d8659778f478883d8da659374096f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacty of 44.48 GiB of which 63.31 MiB is free. Process 623 has 28.62 GiB memory in use. Including non-PyTorch memory, this process has 15.80 GiB memory in use. Of the allocated memory 15.64 GiB is allocated by PyTorch, and 2.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m alpaca_model \u001b[38;5;241m=\u001b[39m \u001b[43mAlpacaInference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/factual_source/lib/python3.11/site-packages/factual_evaluation_source_based-1.0.0-py3.11.egg/experiments/models/AlpacaModel.py:9\u001b[0m, in \u001b[0;36mAlpacaInference.__init__\u001b[0;34m(self, alpaca_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(alpaca_path)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(alpaca_path, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/factual_source/lib/python3.11/site-packages/transformers/modeling_utils.py:2179\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2177\u001b[0m     )\n\u001b[1;32m   2178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/factual_source/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/factual_source/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/factual_source/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/factual_source/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/factual_source/lib/python3.11/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.conda/envs/factual_source/lib/python3.11/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacty of 44.48 GiB of which 63.31 MiB is free. Process 623 has 28.62 GiB memory in use. Including non-PyTorch memory, this process has 15.80 GiB memory in use. Of the allocated memory 15.64 GiB is allocated by PyTorch, and 2.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "alpaca_model = AlpacaInference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df22f8-91fa-4315-95dc-b8083ed82167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(df, \n",
    "              eval_type, \n",
    "              afacts = False, \n",
    "              model = None, \n",
    "              fewshot_examples = None , \n",
    "              fewshot_idx = 0):\n",
    "\n",
    "    \n",
    "    if not afacts:\n",
    "        source_key = 'dialogue'\n",
    "        source_type = 'Dialogue'\n",
    "        \n",
    "    else:\n",
    "        source_key = 'dialogue_atomic_facts'\n",
    "        source_type = 'Source'\n",
    "        \n",
    "    sources = list(df[source_key].values)\n",
    "    summaries = list(df['summary'].values)\n",
    "\n",
    "    response_instruction_dict = {\n",
    "        'response_instr1': [],\n",
    "        'response_instr2': [],\n",
    "        'response_instr3': [],\n",
    "        'labels_instr1': [],\n",
    "        'labels_instr2': [],\n",
    "        'labels_instr3': [],\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    index = 0\n",
    "    for src, summ in tqdm(list(zip(sources, summaries))):\n",
    "        print_prompt = False\n",
    "        if index%100 == 0:\n",
    "            print_prompt = True\n",
    "        else:\n",
    "            print_prompt = False\n",
    "        if eval_type == 'direct_assesment':\n",
    "            responses, labels = PromptBaselines().direct_assessment_zeroshot(src, \n",
    "                                                            summ, \n",
    "                                                            source_type  = source_type, \n",
    "                                                            model = model,\n",
    "                                                            print_prompt = False,\n",
    "                                                            )\n",
    "\n",
    "            response_instruction_dict['response_instr1'] +=  [responses[0]]\n",
    "            response_instruction_dict['response_instr2'] += [responses[1]]\n",
    "            response_instruction_dict['response_instr3'] += [responses[2]]\n",
    "\n",
    "            response_instruction_dict['labels_instr1'] +=  [labels[0]]\n",
    "            response_instruction_dict['labels_instr2'] += [labels[1]]\n",
    "            response_instruction_dict['labels_instr3'] += [labels[2]]\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "        index += 1\n",
    "        if print_prompt:\n",
    "            print('******')\n",
    "            print(labels)\n",
    "            print('RESPONSE', responses ,' ---')\n",
    "            print('*****')\n",
    "            \n",
    "    return response_instruction_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943b645-cf10-4940-9b70-f06b63429ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d3740b-6044-43dd-9b50-8d2df30dc81e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "afacts = False\n",
    "model = alpaca_model\n",
    "while True:\n",
    "    response_instruction_dict_da_alpaca_zs = get_score(df, \n",
    "                eval_type = 'direct_assesment', \n",
    "                  afacts = afacts, \n",
    "                  model = model, \n",
    "                  fewshot_examples = None , \n",
    "                  fewshot_idx = 0)\n",
    "\n",
    "    model_name = 'GPT' if not model else 'Alpaca'\n",
    "    afacts_str = 'Afact' if afacts else 'Dlg'\n",
    "    \n",
    "    # for k ,v in response_instruction_dict_da_alpaca_zs.items():\n",
    "    #     df[f'{k}_{afacts_str}_{model_name}'] = v\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136fba63-a133-495a-9abd-a808546a0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample = df[:5]\n",
    "afacts = True\n",
    "model = alpaca_model\n",
    "\n",
    "response_instruction_dict_da_alpaca_zs = get_score(df, \n",
    "            eval_type = 'direct_assesment', \n",
    "              afacts = afacts, \n",
    "              model = model, \n",
    "              fewshot_examples = None , \n",
    "              fewshot_idx = 0)\n",
    "\n",
    "model_name = 'GPT' if not model_name else 'Alpaca'\n",
    "afacts_str = 'Afact' if afacts else 'Dlg'\n",
    "\n",
    "for k ,v in response_instruction_dict_da_alpaca_zs.items():\n",
    "    df_sample[f'{k}_{afacts_str}_{model_name}'] = v\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "feab33f3-df46-4714-8292-365c11c2d2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                           | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT Decide if the Summary is consistent with the corresponding Dialogue. Note that consistency means all information in the summary is supported by the Dialogue.\n",
      "Answer \"yes\" for consistent and \"no\" for inconsistent.\n",
      "Dialogue: #Person1#: I want to go to china for sight-seeing. What do you think of it, Mum?\n",
      "#Person2#: Why not? China is a wonderful country.\n",
      "#Person1#: Will you go with me, too?\n",
      "#Person2#: No, I'm afraid not now. I'm too busy.\n",
      "Summary: #Person1# asks #Person2# to go to China with #Person1#.\n",
      "Answer:\n",
      "***\n",
      "PROMPT Verify if the Summary aligns with the Dialogue for consistency. Consistency ensures that every detail in the Summary is substantiated by the Dialogue.\n",
      "Answer \"yes\" for consistent and \"no\" for inconsistent. \n",
      "Dialogue: #Person1#: I want to go to china for sight-seeing. What do you think of it, Mum?\n",
      "#Person2#: Why not? China is a wonderful country.\n",
      "#Person1#: Will you go with me, too?\n",
      "#Person2#: No, I'm afraid not now. I'm too busy.\n",
      "Summary: #Person1# asks #Person2# to go to China with #Person1#.\n",
      "Answer:\n",
      "***\n",
      "PROMPT Evaluate the Summary's consistency with the Dialogue by confirming if all information in the summary is supported by the Dialogue.\n",
      "Respond with a yes or no.\n",
      "Dialogue: #Person1#: I want to go to china for sight-seeing. What do you think of it, Mum?\n",
      "#Person2#: Why not? China is a wonderful country.\n",
      "#Person1#: Will you go with me, too?\n",
      "#Person2#: No, I'm afraid not now. I'm too busy.\n",
      "Summary: #Person1# asks #Person2# to go to China with #Person1#.\n",
      "Answer:\n",
      "***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████████████████▊                                                                                                                                               | 1/5 [00:01<00:04,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******\n",
      "[0, 1, 0]\n",
      "RESPONSE ['Yes', 'No', 'Yes']  ---\n",
      "*****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:06<00:00,  1.26s/it]\n",
      "/tmp/ipykernel_17485/4154730743.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample[f'{k}_{afacts_str}_{model_name}'] = v\n",
      "/tmp/ipykernel_17485/4154730743.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample[f'{k}_{afacts_str}_{model_name}'] = v\n",
      "/tmp/ipykernel_17485/4154730743.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample[f'{k}_{afacts_str}_{model_name}'] = v\n",
      "/tmp/ipykernel_17485/4154730743.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample[f'{k}_{afacts_str}_{model_name}'] = v\n",
      "/tmp/ipykernel_17485/4154730743.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample[f'{k}_{afacts_str}_{model_name}'] = v\n",
      "/tmp/ipykernel_17485/4154730743.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample[f'{k}_{afacts_str}_{model_name}'] = v\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>docid</th>\n",
       "      <th>model</th>\n",
       "      <th>nonfactual_spans</th>\n",
       "      <th>evidence</th>\n",
       "      <th>summary</th>\n",
       "      <th>factual_error</th>\n",
       "      <th>error_type</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>origin</th>\n",
       "      <th>...</th>\n",
       "      <th>response_instr3_Afact_Alpaca</th>\n",
       "      <th>labels_instr1_Afact_Alpaca</th>\n",
       "      <th>labels_instr2_Afact_Alpaca</th>\n",
       "      <th>labels_instr3_Afact_Alpaca</th>\n",
       "      <th>response_instr1_Dlg_GPT</th>\n",
       "      <th>response_instr2_Dlg_GPT</th>\n",
       "      <th>response_instr3_Dlg_GPT</th>\n",
       "      <th>labels_instr1_Dlg_GPT</th>\n",
       "      <th>labels_instr2_Dlg_GPT</th>\n",
       "      <th>labels_instr3_Dlg_GPT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test_312</td>\n",
       "      <td>CODS</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>#Person1# asks #Person2# to go to China with #...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>#Person1#: I want to go to china for sight-see...</td>\n",
       "      <td>DialogueSum</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test_312</td>\n",
       "      <td>UniLM</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>person1 wants to go to china for sight - seein...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>#Person1#: I want to go to china for sight-see...</td>\n",
       "      <td>DialogueSum</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test_312</td>\n",
       "      <td>BART</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>##Person1# wants to go to China but #Person2#'...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>#Person1#: I want to go to china for sight-see...</td>\n",
       "      <td>DialogueSum</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>test_312</td>\n",
       "      <td>MV-BART</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>#Person1# wants to go to China for sight-seein...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>#Person1#: I want to go to china for sight-see...</td>\n",
       "      <td>DialogueSum</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>test_312</td>\n",
       "      <td>gpt4-32k-0613</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Person1 expresses a desire to visit China for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>#Person1#: I want to go to china for sight-see...</td>\n",
       "      <td>DialogueSum</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     docid          model nonfactual_spans evidence  \\\n",
       "0           0  test_312           CODS               []       []   \n",
       "1           1  test_312          UniLM               []       []   \n",
       "2           2  test_312           BART               []       []   \n",
       "3           3  test_312        MV-BART               []       []   \n",
       "4           4  test_312  gpt4-32k-0613               []       []   \n",
       "\n",
       "                                             summary  factual_error  \\\n",
       "0  #Person1# asks #Person2# to go to China with #...              0   \n",
       "1  person1 wants to go to china for sight - seein...              0   \n",
       "2  ##Person1# wants to go to China but #Person2#'...              0   \n",
       "3  #Person1# wants to go to China for sight-seein...              0   \n",
       "4  Person1 expresses a desire to visit China for ...              0   \n",
       "\n",
       "  error_type                                           dialogue       origin  \\\n",
       "0         []  #Person1#: I want to go to china for sight-see...  DialogueSum   \n",
       "1         []  #Person1#: I want to go to china for sight-see...  DialogueSum   \n",
       "2         []  #Person1#: I want to go to china for sight-see...  DialogueSum   \n",
       "3         []  #Person1#: I want to go to china for sight-see...  DialogueSum   \n",
       "4         []  #Person1#: I want to go to china for sight-see...  DialogueSum   \n",
       "\n",
       "   ... response_instr3_Afact_Alpaca labels_instr1_Afact_Alpaca  \\\n",
       "0  ...                           No                          1   \n",
       "1  ...                          Yes                          1   \n",
       "2  ...                          Yes                          1   \n",
       "3  ...                          Yes                          1   \n",
       "4  ...                          Yes                          0   \n",
       "\n",
       "  labels_instr2_Afact_Alpaca labels_instr3_Afact_Alpaca  \\\n",
       "0                          1                          1   \n",
       "1                          0                          0   \n",
       "2                          0                          0   \n",
       "3                          0                          0   \n",
       "4                          0                          0   \n",
       "\n",
       "   response_instr1_Dlg_GPT  response_instr2_Dlg_GPT  response_instr3_Dlg_GPT  \\\n",
       "0                      Yes                       No                      Yes   \n",
       "1                      Yes                      Yes                      Yes   \n",
       "2                      Yes                      Yes                      Yes   \n",
       "3                      Yes                      Yes                      Yes   \n",
       "4                      Yes                      Yes                      Yes   \n",
       "\n",
       "  labels_instr1_Dlg_GPT labels_instr2_Dlg_GPT labels_instr3_Dlg_GPT  \n",
       "0                     0                     1                     0  \n",
       "1                     0                     0                     0  \n",
       "2                     0                     0                     0  \n",
       "3                     0                     0                     0  \n",
       "4                     0                     0                     0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_sample = df[:5]\n",
    "afacts = False\n",
    "model = None\n",
    "\n",
    "response_instruction_dict_da_alpaca_zs = get_score(df_sample, \n",
    "            eval_type = 'direct_assesment', \n",
    "              afacts = afacts, \n",
    "              model = model, \n",
    "              fewshot_examples = None , \n",
    "              fewshot_idx = 0)\n",
    "\n",
    "model_name = 'GPT' if not model else 'Alpaca'\n",
    "afacts_str = 'Afact' if afacts else 'Dlg'\n",
    "\n",
    "for k ,v in response_instruction_dict_da_alpaca_zs.items():\n",
    "    df_sample[f'{k}_{afacts_str}_{model_name}'] = v\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d57143f5-44bd-4b61-a6e1-3654142d923b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                           | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT Decide if the Summary is consistent with the corresponding Source. Note that consistency means all information in the summary is supported by the Source.\n",
      "Answer \"yes\" for consistent and \"no\" for inconsistent.\n",
      "Source: Person1 wants to go to China.\n",
      "The purpose of Person1's trip to China is sight-seeing.\n",
      "Person1 asks Mum for her opinion about the trip to China.\n",
      "Person2 thinks China is a wonderful country.\n",
      "Person1 wants Person2/Mum to go with him/her.\n",
      "Person2/Mum cannot go with Person1.\n",
      "The reason Person2/Mum cannot go is because she is too busy.\n",
      "Summary: #Person1# asks #Person2# to go to China with #Person1#.\n",
      "Answer:\n",
      "***\n",
      "PROMPT Verify if the Summary aligns with the Source for consistency. Consistency ensures that every detail in the Summary is substantiated by the Source.\n",
      "Answer \"yes\" for consistent and \"no\" for inconsistent. \n",
      "Source: Person1 wants to go to China.\n",
      "The purpose of Person1's trip to China is sight-seeing.\n",
      "Person1 asks Mum for her opinion about the trip to China.\n",
      "Person2 thinks China is a wonderful country.\n",
      "Person1 wants Person2/Mum to go with him/her.\n",
      "Person2/Mum cannot go with Person1.\n",
      "The reason Person2/Mum cannot go is because she is too busy.\n",
      "Summary: #Person1# asks #Person2# to go to China with #Person1#.\n",
      "Answer:\n",
      "***\n",
      "PROMPT Evaluate the Summary's consistency with the Source by confirming if all information in the summary is supported by the Source.\n",
      "Respond with a yes or no.\n",
      "Source: Person1 wants to go to China.\n",
      "The purpose of Person1's trip to China is sight-seeing.\n",
      "Person1 asks Mum for her opinion about the trip to China.\n",
      "Person2 thinks China is a wonderful country.\n",
      "Person1 wants Person2/Mum to go with him/her.\n",
      "Person2/Mum cannot go with Person1.\n",
      "The reason Person2/Mum cannot go is because she is too busy.\n",
      "Summary: #Person1# asks #Person2# to go to China with #Person1#.\n",
      "Answer:\n",
      "***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████████████████▊                                                                                                                                               | 1/5 [00:01<00:04,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******\n",
      "[0, 0, 0]\n",
      "RESPONSE ['Yes', 'Yes', 'Yes']  ---\n",
      "*****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.12s/it]\n",
      "/tmp/ipykernel_17485/468738456.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample[f'{k}_{afacts_str}_{model_name}'] = v\n",
      "/tmp/ipykernel_17485/468738456.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample[f'{k}_{afacts_str}_{model_name}'] = v\n",
      "/tmp/ipykernel_17485/468738456.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample[f'{k}_{afacts_str}_{model_name}'] = v\n",
      "/tmp/ipykernel_17485/468738456.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample[f'{k}_{afacts_str}_{model_name}'] = v\n",
      "/tmp/ipykernel_17485/468738456.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample[f'{k}_{afacts_str}_{model_name}'] = v\n",
      "/tmp/ipykernel_17485/468738456.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample[f'{k}_{afacts_str}_{model_name}'] = v\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>docid</th>\n",
       "      <th>model</th>\n",
       "      <th>nonfactual_spans</th>\n",
       "      <th>evidence</th>\n",
       "      <th>summary</th>\n",
       "      <th>factual_error</th>\n",
       "      <th>error_type</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>origin</th>\n",
       "      <th>...</th>\n",
       "      <th>response_instr3_Dlg_GPT</th>\n",
       "      <th>labels_instr1_Dlg_GPT</th>\n",
       "      <th>labels_instr2_Dlg_GPT</th>\n",
       "      <th>labels_instr3_Dlg_GPT</th>\n",
       "      <th>response_instr1_Afact_GPT</th>\n",
       "      <th>response_instr2_Afact_GPT</th>\n",
       "      <th>response_instr3_Afact_GPT</th>\n",
       "      <th>labels_instr1_Afact_GPT</th>\n",
       "      <th>labels_instr2_Afact_GPT</th>\n",
       "      <th>labels_instr3_Afact_GPT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test_312</td>\n",
       "      <td>CODS</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>#Person1# asks #Person2# to go to China with #...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>#Person1#: I want to go to china for sight-see...</td>\n",
       "      <td>DialogueSum</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test_312</td>\n",
       "      <td>UniLM</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>person1 wants to go to china for sight - seein...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>#Person1#: I want to go to china for sight-see...</td>\n",
       "      <td>DialogueSum</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test_312</td>\n",
       "      <td>BART</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>##Person1# wants to go to China but #Person2#'...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>#Person1#: I want to go to china for sight-see...</td>\n",
       "      <td>DialogueSum</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>test_312</td>\n",
       "      <td>MV-BART</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>#Person1# wants to go to China for sight-seein...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>#Person1#: I want to go to china for sight-see...</td>\n",
       "      <td>DialogueSum</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>test_312</td>\n",
       "      <td>gpt4-32k-0613</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Person1 expresses a desire to visit China for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>#Person1#: I want to go to china for sight-see...</td>\n",
       "      <td>DialogueSum</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     docid          model nonfactual_spans evidence  \\\n",
       "0           0  test_312           CODS               []       []   \n",
       "1           1  test_312          UniLM               []       []   \n",
       "2           2  test_312           BART               []       []   \n",
       "3           3  test_312        MV-BART               []       []   \n",
       "4           4  test_312  gpt4-32k-0613               []       []   \n",
       "\n",
       "                                             summary  factual_error  \\\n",
       "0  #Person1# asks #Person2# to go to China with #...              0   \n",
       "1  person1 wants to go to china for sight - seein...              0   \n",
       "2  ##Person1# wants to go to China but #Person2#'...              0   \n",
       "3  #Person1# wants to go to China for sight-seein...              0   \n",
       "4  Person1 expresses a desire to visit China for ...              0   \n",
       "\n",
       "  error_type                                           dialogue       origin  \\\n",
       "0         []  #Person1#: I want to go to china for sight-see...  DialogueSum   \n",
       "1         []  #Person1#: I want to go to china for sight-see...  DialogueSum   \n",
       "2         []  #Person1#: I want to go to china for sight-see...  DialogueSum   \n",
       "3         []  #Person1#: I want to go to china for sight-see...  DialogueSum   \n",
       "4         []  #Person1#: I want to go to china for sight-see...  DialogueSum   \n",
       "\n",
       "   ... response_instr3_Dlg_GPT labels_instr1_Dlg_GPT labels_instr2_Dlg_GPT  \\\n",
       "0  ...                     Yes                     0                     1   \n",
       "1  ...                     Yes                     0                     0   \n",
       "2  ...                     Yes                     0                     0   \n",
       "3  ...                     Yes                     0                     0   \n",
       "4  ...                     Yes                     0                     0   \n",
       "\n",
       "  labels_instr3_Dlg_GPT  response_instr1_Afact_GPT  response_instr2_Afact_GPT  \\\n",
       "0                     0                        Yes                        Yes   \n",
       "1                     0                        Yes                        Yes   \n",
       "2                     0                        Yes                        Yes   \n",
       "3                     0                        Yes                        Yes   \n",
       "4                     0                        Yes                        Yes   \n",
       "\n",
       "   response_instr3_Afact_GPT labels_instr1_Afact_GPT labels_instr2_Afact_GPT  \\\n",
       "0                        Yes                       0                       0   \n",
       "1                        Yes                       0                       0   \n",
       "2                        Yes                       0                       0   \n",
       "3                        Yes                       0                       0   \n",
       "4                        Yes                       0                       0   \n",
       "\n",
       "  labels_instr3_Afact_GPT  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_sample = df[:5]\n",
    "afacts = True\n",
    "model = None\n",
    "\n",
    "response_instruction_dict_da_alpaca_zs = get_score(df_sample, \n",
    "            eval_type = 'direct_assesment', \n",
    "              afacts = afacts, \n",
    "              model = model, \n",
    "              fewshot_examples = None , \n",
    "              fewshot_idx = 0)\n",
    "\n",
    "model_name = 'GPT' if not model else 'Alpaca'\n",
    "afacts_str = 'Afact' if afacts else 'Dlg'\n",
    "\n",
    "for k ,v in response_instruction_dict_da_alpaca_zs.items():\n",
    "    df_sample[f'{k}_{afacts_str}_{model_name}'] = v\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d86e8455-46cc-4597-8428-c217bd6d620b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'docid', 'model', 'nonfactual_spans', 'evidence',\n",
       "       'summary', 'factual_error', 'error_type', 'dialogue', 'origin',\n",
       "       'dialogue_atomic_facts', 'response_instr1_Dlg_Alpaca',\n",
       "       'response_instr2_Dlg_Alpaca', 'response_instr3_Dlg_Alpaca',\n",
       "       'labels_instr1_Dlg_Alpaca', 'labels_instr2_Dlg_Alpaca',\n",
       "       'labels_instr3_Dlg_Alpaca', 'response_instr1_Afact_Alpaca',\n",
       "       'response_instr2_Afact_Alpaca', 'response_instr3_Afact_Alpaca',\n",
       "       'labels_instr1_Afact_Alpaca', 'labels_instr2_Afact_Alpaca',\n",
       "       'labels_instr3_Afact_Alpaca', 'response_instr1_Dlg_GPT',\n",
       "       'response_instr2_Dlg_GPT', 'response_instr3_Dlg_GPT',\n",
       "       'labels_instr1_Dlg_GPT', 'labels_instr2_Dlg_GPT',\n",
       "       'labels_instr3_Dlg_GPT', 'response_instr1_Afact_GPT',\n",
       "       'response_instr2_Afact_GPT', 'response_instr3_Afact_GPT',\n",
       "       'labels_instr1_Afact_GPT', 'labels_instr2_Afact_GPT',\n",
       "       'labels_instr3_Afact_GPT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da8af3-3cb0-49bb-b21d-320c5296680a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Factual Eval(Abridge)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
