{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "666f4956-2fa0-47be-b9e1-a72a9d7c5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re, string\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm \n",
    "from experiments.RQ1.utils import get_chatgpt_response, get_atomic_facts_gpt\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fcb066d8-43c6-4853-9826-aff8ebd525d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filter = {\n",
    "    'FacEval': ['bart_large', 'co-ref bart large', 'condigsum bart large','gpt4-32k-0613','mv-bart_large', 'alpaca-13b'],\n",
    "    'SAMSum': ['BART', 'CODS', 'MV-BART', 'UniLM', 'gpt4-32k-0613', 'alpaca-13b'],\n",
    "    'DialogueSum': ['BART', 'CODS', 'MV-BART', 'UniLM', 'gpt4-32k-0613', 'alpaca-13b']\n",
    "\n",
    "}\n",
    "def read_filter(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df_datasets = []\n",
    "    unique_datasets = list(set(df['origin'].values))\n",
    "    for dataset in unique_datasets: \n",
    "        df_origin = df[df['origin'] == dataset]\n",
    "        df_origin = df_origin[df_origin['model'].isin(model_filter[dataset])]\n",
    "        # print(len(df_origin))\n",
    "        unique_docids = list(set(df_origin['docid'].values))\n",
    "        #### test ###\n",
    "        num_models = []\n",
    "        for udocid in unique_docids:\n",
    "            df_docid = df_origin[df_origin['docid'] == udocid]\n",
    "            num_models.append(len(list(set(df_docid['model'].values))))\n",
    "        # print(df_docid)\n",
    "        assert(len(set(num_models)) == 1) \n",
    "        df_datasets.append(df_origin)\n",
    "    df_filtered = pd.concat(df_datasets)\n",
    "    assert(len(df_filtered) <= len(df))\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e02a1-19b0-4d9a-9b5b-d99eea503069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7de5956b-1a23-4f4e-bf05-104aa5fcd352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>docid</th>\n",
       "      <th>model</th>\n",
       "      <th>nonfactual_spans</th>\n",
       "      <th>evidence</th>\n",
       "      <th>summary</th>\n",
       "      <th>factual_error</th>\n",
       "      <th>error_type</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>origin</th>\n",
       "      <th>dialogue_atomic_facts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>test_133</td>\n",
       "      <td>BART</td>\n",
       "      <td>['They ']</td>\n",
       "      <td>[]</td>\n",
       "      <td>#Person1# and #Person2# talk about the heavy s...</td>\n",
       "      <td>1</td>\n",
       "      <td>['Intrinsic_Error']</td>\n",
       "      <td>#Person1#: It was a heavy storm last night, wa...</td>\n",
       "      <td>DialogueSum</td>\n",
       "      <td>There was a heavy storm last night.\\nThe wind ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     docid model nonfactual_spans evidence  \\\n",
       "7           7  test_133  BART        ['They ']       []   \n",
       "\n",
       "                                             summary  factual_error  \\\n",
       "7  #Person1# and #Person2# talk about the heavy s...              1   \n",
       "\n",
       "            error_type                                           dialogue  \\\n",
       "7  ['Intrinsic_Error']  #Person1#: It was a heavy storm last night, wa...   \n",
       "\n",
       "        origin                              dialogue_atomic_facts  \n",
       "7  DialogueSum  There was a heavy storm last night.\\nThe wind ...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_filter('/home/ramprasad.sa/factual_evaluation_source_based/annotations/xformer_llm_annotated.csv')\n",
    "df = df[df['origin'] != 'FacEval']\n",
    "df[df['factual_error'] == 1][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e0a6479f-da54-4b5d-b7c3-3bd872137f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ramprasad.sa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string \n",
    "import spacy \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from random import sample\n",
    "import spacy\n",
    "from thefuzz import fuzz\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def postprocess(text):\n",
    "    text = text.strip(string.punctuation).lower()\n",
    "    return text\n",
    "    \n",
    "\n",
    "def calculate_f1(matched, pred_spans, annotated_spans):\n",
    "    found_pred_spans = set([each[0] for each in matched])\n",
    "    found_rec_spans = set([each[1] for each in matched])\n",
    "    \n",
    "    precision = len(found_pred_spans)/len(pred_spans)\n",
    "    recall  = len(found_rec_spans)/len(annotated_spans)\n",
    "    \n",
    "    if precision + recall > 0:\n",
    "        f1_score = (2 * precision * recall)/(precision + recall)\n",
    "    else:\n",
    "        f1_score = 0\n",
    "    return f1_score\n",
    "    \n",
    "def get_f1_scores(all_pred_spans, nonfactual_spans):\n",
    "    \n",
    "    matched_pred_fuzzy = []\n",
    "    matched_pred_exact = []\n",
    "    for pred_span in all_pred_spans:\n",
    "        for ref_span in nonfactual_spans:\n",
    "            fuzzy_score = fuzz.partial_ratio(pred_span, ref_span)\n",
    "            # print(pred_span, ref_span, fuzzy_score)\n",
    "            if fuzzy_score > 80:\n",
    "                matched_pred_fuzzy.append((pred_span, ref_span))\n",
    "            if pred_span == ref_span:\n",
    "                matched_pred_exact.append((pred_span, ref_span))\n",
    "            \n",
    "    matched_pred_fuzzy = list(set(matched_pred_fuzzy))\n",
    "    matched_pred_exact = list(set(matched_pred_exact))\n",
    "    # print('MATCHED LEN AND STRICT', matched_pred_fuzzy, matched_pred_exact)\n",
    "    # print('PRED, ANN', all_pred_spans, nonfactual_spans)\n",
    "    \n",
    "    f1_score_lenient = calculate_f1(matched_pred_fuzzy, all_pred_spans, nonfactual_spans)\n",
    "    f1_score_strict = calculate_f1(matched_pred_exact, all_pred_spans, nonfactual_spans)\n",
    "    return f1_score_lenient, f1_score_strict\n",
    "    \n",
    "\n",
    "def make_masked_sentences(summ):\n",
    "    # summ = summ.lower()\n",
    "    instruction = 'Extract all grammatical units including subjects in the sentence. List each in a new line'\n",
    "    prompt_template = f\"{{instruction}}:\\nSentence:{{sent}}\\nAnswer:\"\n",
    "    \n",
    "    summ = nlp(summ)\n",
    "    summ_sentences = [each.text.lower() for each in summ.sents]\n",
    "    \n",
    "    summ_sent_mask_infill = []\n",
    "    for sent_idx, sent in enumerate(summ_sentences):\n",
    "        prompt = prompt_template.format(instruction = instruction, sent = sent)\n",
    "        # print(prompt)\n",
    "        response = get_chatgpt_response(prompt)\n",
    "        spans = response.split('\\n')\n",
    "        # print(sent, spans)\n",
    "        for span in spans:\n",
    "            masked_sent = sent.replace(' '+span+ ' ', ' ___ ')\n",
    "            if '___' in masked_sent:\n",
    "                summ_blank = summ_sentences[:sent_idx] + [masked_sent] + summ_sentences[sent_idx + 1:]\n",
    "                summ_sent_mask_infill.append((' '.join(summ_blank), masked_sent, span))\n",
    "    return summ_sent_mask_infill\n",
    "\n",
    "\n",
    "def process_gpt_response_infill(response):\n",
    "    if \"None\" in response:\n",
    "        response = [response]\n",
    "        \n",
    "    elif len(response.split('\\n')) > 1:\n",
    "        response = response.split('\\n')\n",
    "        \n",
    "    elif len(response.split(',')) > 1:\n",
    "        response = response.split(',')\n",
    "        \n",
    "    else:\n",
    "        response = [response]\n",
    "    # response = [re.sub('[1-9]', '', each) for each in response]\n",
    "    # response = [each.strip(string.punctuation).strip() for each in response]\n",
    "    return response\n",
    "\n",
    "class GPTPrompttInfill():\n",
    "    def __init__(self):\n",
    "        self.gpt_instructions = {\n",
    "            'infill': '''Given below is a dialogue snippet and a its summary. Complete the summary using information from the dialogue.\\nLimit your answer to 1-3 words. If the summary cannot be completed accurately, respond with \"None\".''',\n",
    "            'entailment': 'Decide if the hypothesis is consistent with the corresponding premise.'\n",
    "            # The dialogue snippet above is provided for context',\n",
    "    \n",
    "        }\n",
    "        self.gpt_prompt_templates = {\n",
    "            'infill': f\"{self.gpt_instructions['infill']}\\nDialogue: {{source}}\\nSummary: {{summary}}\\nAnswer Span:\",\n",
    "            \"infill_fewshot\" : f\"{self.gpt_instructions['infill']}\\nSource: {{source}}\\n{{fewshot_str}}\\nSummary: {{summary}}\\nAnswer:\",\n",
    "            'entailment': f\"{self.gpt_instructions['entailment']}\\nPremise: {{premise}}\\nHypothesis: {{hypothesis}}\\nRate on a scale of 1-100:\"\n",
    "        }\n",
    "\n",
    "    def check_entailment_score_gpt(self, source ,premise, hypothesis):\n",
    "        \n",
    "        prompt = self.gpt_prompt_templates['entailment'].format(\n",
    "                                       premise = premise,\n",
    "                                       hypothesis = hypothesis)\n",
    "        # print(prompt)\n",
    "        answer = get_chatgpt_response(prompt)\n",
    "        answer = re.findall(r'\\d+', answer)\n",
    "        if not answer:\n",
    "            return 0\n",
    "        else:\n",
    "            answer = answer[0]\n",
    "        # if eval(answer) < 90:\n",
    "        #     print(prompt)\n",
    "        return eval(answer)\n",
    "\n",
    "    def phrase_similarity_infilled(self, summary_span, infill_answers, masked_sentence, dlg):\n",
    "        summary_sentence = masked_sentence.replace('___', summary_span)\n",
    "    \n",
    "        all_scores = []\n",
    "        for infill in infill_answers:\n",
    "            if infill.lower().strip() == 'none':\n",
    "                return True\n",
    "            infill_sentence = masked_sentence.replace('___', infill)\n",
    "            entailment_score = self.check_entailment_score_gpt(source = dlg, \n",
    "                                                               premise = infill_sentence, \n",
    "                                                               hypothesis= summary_sentence)\n",
    "\n",
    "            overlap = set(summary_span.lower().split(' ')).intersection(set(infill.lower().split(' '))) \n",
    "            precision = len(overlap)/len(summary_span.lower().split(' '))\n",
    "            recall = len(overlap)/len(infill.lower().split(' '))\n",
    "            f1_score = 0\n",
    "            if precision + recall > 0:\n",
    "                f1_score = (2* (precision * recall)) / (precision + recall)\n",
    "\n",
    "            \n",
    "            # if (f1_score < 0.6) and (entailment_score < 90):\n",
    "                \n",
    "            #     print('SENT', masked_sentence)\n",
    "            #     print('SPAN', summary_span)\n",
    "            #     print('RECC', infill)\n",
    "            #     print(f1_score, entailment_score)\n",
    "            #     print('**')\n",
    "            if (f1_score >= 0.6) or (entailment_score >= 90):\n",
    "                # print('SENT', masked_sentence)\n",
    "                # print('SPAN', summary_span)\n",
    "                # print('RECC', infill)\n",
    "                # print(f1_score, entailment_score)\n",
    "                # print('**')\n",
    "                return True\n",
    "        # print('='*13)\n",
    "        return False\n",
    "\n",
    "    def infill_spans(self, dlg, summ, ):\n",
    "        all_masked_results = make_masked_sentences(summ)\n",
    "        answers = []\n",
    "        nonfactual_sent_span = []\n",
    "        print_sample = 1\n",
    "        for summary, sentence, answer in all_masked_results:   \n",
    "            \n",
    "            prompt = self.gpt_prompt_templates['infill'].format(source = dlg, \n",
    "                                                                summary = sentence)\n",
    "                \n",
    "            if print_sample < 1:\n",
    "                print('PROMPT', prompt)\n",
    "                # print_sample = False\n",
    "                print_sample += 1\n",
    "            # print(sentence, answer)\n",
    "            gpt_response = get_chatgpt_response(prompt)\n",
    "            response = process_gpt_response_infill(gpt_response)\n",
    "\n",
    "            if not self.phrase_similarity_infilled(answer, response, sentence\n",
    "                                                   , dlg):\n",
    "                answers += [answer]\n",
    "                # print('===')\n",
    "\n",
    "        return answers\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "92dc5482-85b8-465b-a35c-3de2db097f1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### sentence level and entailment score 90 \n",
    "\n",
    "# df= df[df['factual_error'] == 1]\n",
    "# for idx, row in df[:30].iterrows():\n",
    "#     dlg = row['dialogue']\n",
    "#     summ = row['summary']\n",
    "#     pred_spans = GPTPrompttInfill().infill_spans(dlg, summ)\n",
    "#     print('Dialogue', dlg)\n",
    "#     print('SUmmary', summ)\n",
    "#     print('PRED', pred_spans)\n",
    "#     print('ANN', row['nonfactual_spans'])\n",
    "#     print('='*13)\n",
    "\n",
    "def score(all_pred_spans, nonfactual_spans):\n",
    "    if not all_pred_spans:\n",
    "        if not nonfactual_spans:\n",
    "            f1_score_lenient = 1\n",
    "            f1_score_strict = 1\n",
    "        else:\n",
    "            f1_score_lenient = 0\n",
    "            f1_score_strict = 0\n",
    "            \n",
    "    else:\n",
    "        if not nonfactual_spans:\n",
    "            f1_score_lenient = 0\n",
    "            f1_score_strict = 0\n",
    "        else:\n",
    "            f1_score_lenient, f1_score_strict = get_f1_scores(all_pred_spans, nonfactual_spans)\n",
    "    return f1_score_lenient, f1_score_strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "17d00726-e7e3-45cd-9b89-b4d4fb33c86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1050/1050 [4:27:00<00:00, 15.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# df= df[df['factual_error'] == 1]\n",
    "span_f1_lenient_scores = []\n",
    "span_f1_strict_scores = []\n",
    "for idx, row in tqdm(df.iterrows(), total = len(df)):\n",
    "    dlg = row['dialogue']\n",
    "    summ = row['summary']\n",
    "    nonfactual_spans = row['nonfactual_spans']\n",
    "    pred_spans = GPTPrompttInfill().infill_spans(dlg, summ)\n",
    "    span_f1_lenient, span_f1_strict = score(list(set(pred_spans)), eval(nonfactual_spans))\n",
    "    span_f1_lenient_scores.append(span_f1_lenient)\n",
    "    span_f1_strict_scores.append(span_f1_strict)\n",
    "    # f1_scores_len += [get_f1_scores(pred_spans, nonfactual_spans)]\n",
    "    # print('Dialogue', dlg)\n",
    "    # print('SUmmary', summ)\n",
    "    # print('PRED', pred_spans)\n",
    "    # print('ANN', row['nonfactual_spans'])\n",
    "    # print(span_f1_lenient)\n",
    "    # print('='*13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3b7b5f19-b187-452c-8209-9714ef041645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47826199071686437"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(span_f1_lenient_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "756f8a69-5745-45c7-9fc3-264191167b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bill ___ happy because he made a move and tells #person1# his roommate, brain locker, ___ happy today.',\n",
       "  'bill ___ happy because he made a move and tells #person1# his roommate, brain locker, ___ happy today.',\n",
       "  'is'),\n",
       " ('bill is ___ because he made a move and tells #person1# his roommate, brain locker, is ___ today.',\n",
       "  'bill is ___ because he made a move and tells #person1# his roommate, brain locker, is ___ today.',\n",
       "  'happy'),\n",
       " ('bill is happy ___ he made a move and tells #person1# his roommate, brain locker, is happy today.',\n",
       "  'bill is happy ___ he made a move and tells #person1# his roommate, brain locker, is happy today.',\n",
       "  'because'),\n",
       " ('bill is happy because ___ made a move and tells #person1# his roommate, brain locker, is happy today.',\n",
       "  'bill is happy because ___ made a move and tells #person1# his roommate, brain locker, is happy today.',\n",
       "  'he'),\n",
       " ('bill is happy because he ___ a move and tells #person1# his roommate, brain locker, is happy today.',\n",
       "  'bill is happy because he ___ a move and tells #person1# his roommate, brain locker, is happy today.',\n",
       "  'made'),\n",
       " ('bill is happy because he made ___ and tells #person1# his roommate, brain locker, is happy today.',\n",
       "  'bill is happy because he made ___ and tells #person1# his roommate, brain locker, is happy today.',\n",
       "  'a move'),\n",
       " ('bill is happy because he made a move ___ tells #person1# his roommate, brain locker, is happy today.',\n",
       "  'bill is happy because he made a move ___ tells #person1# his roommate, brain locker, is happy today.',\n",
       "  'and'),\n",
       " ('bill is happy because he made a move and ___ #person1# his roommate, brain locker, is happy today.',\n",
       "  'bill is happy because he made a move and ___ #person1# his roommate, brain locker, is happy today.',\n",
       "  'tells'),\n",
       " ('bill is happy because he made a move and tells ___ his roommate, brain locker, is happy today.',\n",
       "  'bill is happy because he made a move and tells ___ his roommate, brain locker, is happy today.',\n",
       "  '#person1#'),\n",
       " ('bill ___ because he made a move and tells #person1# his roommate, brain locker, ___ today.',\n",
       "  'bill ___ because he made a move and tells #person1# his roommate, brain locker, ___ today.',\n",
       "  'is happy')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_masked_sentences(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1058a-a997-4859-b5ff-72ef98e93a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c79c00-935a-4c1d-af6f-ba4061df24ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_masked_cand(all_masked_results, seen_last_idx):\n",
    "#     masked_check_cand_ind = -1\n",
    "#     masked_check_cand = None\n",
    "    \n",
    "#     for each_idx, each in enumerate(all_masked_results):\n",
    "#         blank_idx = [widx for widx, word in enumerate(each[0].split(' ')) if '<BLANK>' in word][0]\n",
    "#         # print(blank_idx)\n",
    "#         if blank_idx >= seen_last_idx:\n",
    "#             masked_check_cand_ind = each_idx\n",
    "#             masked_check_cand = each\n",
    "#             break\n",
    "#     return masked_check_cand, masked_check_cand_ind\n",
    "        \n",
    "# def get_mask_infill_errors(dlg, summ):\n",
    "#     seen_last_idx = 0\n",
    "#     updated_summ = summ\n",
    "    \n",
    "#     all_masked_results = make_masked_sentences( updated_summ)\n",
    "#     masked_check_cand, masked_check_cand_ind = get_masked_cand(all_masked_results, seen_last_idx) \n",
    "#     # print(masked_check_cand)\n",
    "#     answers = []\n",
    "#     while masked_check_cand:\n",
    "#         print('SEEN', seen_last_idx)\n",
    "#         masked_summ = masked_check_cand[0]\n",
    "#         masked_sent = masked_check_cand[1]\n",
    "#         masked_span = masked_check_cand[2]\n",
    "        \n",
    "#         prompt = GPTPrompttInfill().gpt_prompt_templates['infill'].format(source = dlg,suggested_span = masked_span,\n",
    "#                                                                 summary = updated_summ)\n",
    "#         # print(prompt)\n",
    "#         gpt_response = get_chatgpt_response(prompt)\n",
    "#         response = process_gpt_response_infill(gpt_response)\n",
    "#         if not GPTPrompttInfill().phrase_similarity_infilled(masked_span, response[:1], masked_summ):\n",
    "#             updated_summ = masked_summ.replace('<BLANK>', response[0])\n",
    "#             answers.append(masked_span)\n",
    "#         seen_last_idx = [idx for idx, word in enumerate(masked_summ.split(' ')) if '<BLANK>' in word][0]\n",
    "#         seen_last_idx += 1\n",
    "#         all_masked_results = make_masked_sentences( updated_summ)\n",
    "#         masked_check_cand, masked_check_cand_ind = get_masked_cand(all_masked_results, seen_last_idx)\n",
    "        \n",
    "#     print(answers , updated_summ)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a451cb35-174c-4010-b03a-500c003e4dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEEN 0\n",
      "SENT <BLANK> drives person2 to the french garden restaurant . person3 orders a bottle of water , a tuna fish sandwich , and vegetable soup .\n",
      "SPAN person1\n",
      "RECC person2\n",
      "0 1\n",
      "**\n",
      "SEEN 1\n",
      "SENT person2 <BLANK> person2 to the french garden restaurant . person3 orders a bottle of water , a tuna fish sandwich , and vegetable soup .\n",
      "SPAN drives\n",
      "RECC None\n",
      "0 2\n",
      "**\n",
      "SEEN 2\n",
      "SEEN 4\n",
      "SENT person2 none person2 to the french garden restaurant . <BLANK> orders a bottle of water , a tuna fish sandwich , and vegetable soup .\n",
      "SPAN person3\n",
      "RECC person2\n",
      "0 1\n",
      "**\n",
      "SEEN 10\n",
      "SENT person2 none person2 to the french garden restaurant . person2 <BLANK> a bottle of water , a tuna fish sandwich , and vegetable soup .\n",
      "SPAN orders\n",
      "RECC went\n",
      "0 2\n",
      "**\n",
      "SEEN 11\n",
      "SEEN 12\n",
      "SENT person2 none person2 to the french garden restaurant . person2 went a bottle of water , <BLANK> , and vegetable soup .\n",
      "SPAN a tuna fish sandwich\n",
      "RECC a bottle of water\n",
      "0.25 1\n",
      "**\n",
      "SEEN 17\n",
      "SENT person2 none person2 to the french garden restaurant . person2 went a bottle of water , a bottle of water , <BLANK> vegetable soup .\n",
      "SPAN and\n",
      "RECC a tuna fish sandwich\n",
      "0 1\n",
      "**\n",
      "SEEN 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df[9:13].iterrows():\n",
    "    dlg = row['dialogue']\n",
    "    summ = row['summary']  \n",
    "    get_mask_infill_errors(dlg, summ)\n",
    "\n",
    "# all_masked_results = make_masked_sentences(summ)\n",
    "# for summary, sentence, answer in all_masked_results:   \n",
    "#     prompt = GPTPrompttInfill().gpt_prompt_templates['infill'].format(source = dlg, summary = summary)\n",
    "    \n",
    "#     gpt_response = get_chatgpt_response(prompt)\n",
    "#     response = process_gpt_response_infilled(gpt_response)\n",
    "#     print(GPTPrompttInfill().phrase_similarity_infilled(answer, response, sentence))\n",
    "\n",
    "# get_mask_infill_errors(dlg, summ)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a0e702ad-f423-40aa-87a6-96f585619a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['she her she ', 'having a lawyer ']\""
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['nonfactual_spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e552642c-8f9d-4bc4-b3e9-a3c1623fabc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vet tells person1 she usually eats a cucumber and go to bed to deal with stress and depression . her favorite part of having a daughter is having a lawyer . vet tells person1 she wants to be a lawyer and wants to start small .'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_masked_cand(all_masked_results, seen_last_idx):\n",
    "    masked_check_cand_ind = -1\n",
    "    masked_check_cand = None\n",
    "    \n",
    "    for each_idx, each in enumerate(all_masked_results):\n",
    "        blank_idx = [widx for widx, word in enumerate(each[0].split(' ')) if '<BLANK>' in word][0]\n",
    "        # print(blank_idx)\n",
    "        if blank_idx >= seen_last_idx:\n",
    "            masked_check_cand_ind = each_idx\n",
    "            masked_check_cand = each\n",
    "            break\n",
    "    return masked_check_cand, masked_check_cand_ind\n",
    "        \n",
    "def get_mask_infill_errors(dlg, summ):\n",
    "    seen_last_idx = 0\n",
    "    updated_summ = summ\n",
    "    \n",
    "    all_masked_results = make_masked_sentences( updated_summ)\n",
    "    masked_check_cand, masked_check_cand_ind = get_masked_cand(all_masked_results, seen_last_idx) \n",
    "    # print(masked_check_cand)\n",
    "    answers = []\n",
    "    while masked_check_cand:\n",
    "        print('SEEN', seen_last_idx)\n",
    "        masked_summ = masked_check_cand[0]\n",
    "        masked_sent = masked_check_cand[1]\n",
    "        masked_span = masked_check_cand[2]\n",
    "        prompt = GPTPrompttInfill().gpt_prompt_templates['infill'].format(source = dlg, summary = masked_summ)\n",
    "        # print(prompt)\n",
    "        gpt_response = get_chatgpt_response(prompt)\n",
    "        response = process_gpt_response_infilled(gpt_response)\n",
    "        if not GPTPrompttInfill().phrase_similarity_infilled(masked_span, response[:1], masked_summ):\n",
    "            updated_summ = masked_summ.replace('<BLANK>', response[0])\n",
    "            answers.append(masked_span)\n",
    "        seen_last_idx = [idx for idx, word in enumerate(masked_summ.split(' ')) if '<BLANK>' in word][0]\n",
    "        seen_last_idx += 1\n",
    "        all_masked_results = make_masked_sentences( updated_summ)\n",
    "        masked_check_cand, masked_check_cand_ind = get_masked_cand(all_masked_results, seen_last_idx)\n",
    "        \n",
    "    print(answers , updated_summ)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Factual Eval(Abridge)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
