{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47d6cec-b40f-4fab-9ac9-6572162ef3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from error_config import error_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15dab99d-584f-4fe2-b282-84c8d403e089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformer = pd.read_csv('/home/sanjana/factual_evaluation_source_based/datasets/scored/dialogue_aggrefact_scored.csv')\n",
    "\n",
    "df_llm = pd.read_csv('/home/sanjana/factual_evaluation_source_based/datasets/llm_annotations/sample/sample_annotations_sanjana.csv')\n",
    "# df_llm = df_llm.sort_values('uuid').drop_duplicates(['docid'], keep='last')\n",
    "\n",
    "len(df_llm)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb56bb86-7751-44db-b46e-b87541660067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>DocID</th>\n",
       "      <th>Dialogue</th>\n",
       "      <th>Model</th>\n",
       "      <th>Summary</th>\n",
       "      <th>w/ Error</th>\n",
       "      <th>CorefE</th>\n",
       "      <th>CorefE_text</th>\n",
       "      <th>CircE</th>\n",
       "      <th>...</th>\n",
       "      <th>PredE_text</th>\n",
       "      <th>SubjObjE</th>\n",
       "      <th>SubjObjE_text</th>\n",
       "      <th>OtherE</th>\n",
       "      <th>OtherE_text</th>\n",
       "      <th>LinkE</th>\n",
       "      <th>LinkE_text</th>\n",
       "      <th>origin</th>\n",
       "      <th>SummaC-ZS_score</th>\n",
       "      <th>SummaC-Conv_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13809941</td>\n",
       "      <td>Thelma: i dont have anything to wear\\nLouisa: ...</td>\n",
       "      <td>human_ref</td>\n",
       "      <td>Louisa will lend Thelma her red velvet dress.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FacEval</td>\n",
       "      <td>0.608673</td>\n",
       "      <td>0.224066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13809941</td>\n",
       "      <td>Thelma: i dont have anything to wear\\nLouisa: ...</td>\n",
       "      <td>bart_large</td>\n",
       "      <td>Thelma doesn't have anything to wear. Louisa w...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FacEval</td>\n",
       "      <td>0.271927</td>\n",
       "      <td>0.335007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13809941</td>\n",
       "      <td>Thelma: i dont have anything to wear\\nLouisa: ...</td>\n",
       "      <td>mv-bart_large</td>\n",
       "      <td>Thelma doesn't have anything to wear. Louisa w...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FacEval</td>\n",
       "      <td>0.271927</td>\n",
       "      <td>0.335007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>13809941</td>\n",
       "      <td>Thelma: i dont have anything to wear\\nLouisa: ...</td>\n",
       "      <td>co-ref bart large</td>\n",
       "      <td>Thelma doesn't have anything to wear. Louisa w...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FacEval</td>\n",
       "      <td>0.090210</td>\n",
       "      <td>0.329921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>13809941</td>\n",
       "      <td>Thelma: i dont have anything to wear\\nLouisa: ...</td>\n",
       "      <td>condigsum bart large</td>\n",
       "      <td>Louisa will bring Thelma her red velvet dress.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FacEval</td>\n",
       "      <td>0.374329</td>\n",
       "      <td>0.224066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0     DocID  \\\n",
       "0             0           0  13809941   \n",
       "1             1           1  13809941   \n",
       "2             2           2  13809941   \n",
       "3             3           3  13809941   \n",
       "4             4           4  13809941   \n",
       "\n",
       "                                            Dialogue                 Model  \\\n",
       "0  Thelma: i dont have anything to wear\\nLouisa: ...             human_ref   \n",
       "1  Thelma: i dont have anything to wear\\nLouisa: ...            bart_large   \n",
       "2  Thelma: i dont have anything to wear\\nLouisa: ...         mv-bart_large   \n",
       "3  Thelma: i dont have anything to wear\\nLouisa: ...     co-ref bart large   \n",
       "4  Thelma: i dont have anything to wear\\nLouisa: ...  condigsum bart large   \n",
       "\n",
       "                                             Summary  w/ Error  CorefE  \\\n",
       "0      Louisa will lend Thelma her red velvet dress.         0     NaN   \n",
       "1  Thelma doesn't have anything to wear. Louisa w...         0     NaN   \n",
       "2  Thelma doesn't have anything to wear. Louisa w...         0     NaN   \n",
       "3  Thelma doesn't have anything to wear. Louisa w...         0     NaN   \n",
       "4     Louisa will bring Thelma her red velvet dress.         0     NaN   \n",
       "\n",
       "  CorefE_text  CircE  ... PredE_text  SubjObjE SubjObjE_text  OtherE  \\\n",
       "0         NaN    NaN  ...        NaN       NaN           NaN     0.0   \n",
       "1         NaN    NaN  ...        NaN       NaN           NaN     0.0   \n",
       "2         NaN    NaN  ...        NaN       NaN           NaN     0.0   \n",
       "3         NaN    NaN  ...        NaN       NaN           NaN     0.0   \n",
       "4         NaN    NaN  ...        NaN       NaN           NaN     0.0   \n",
       "\n",
       "  OtherE_text  LinkE LinkE_text   origin SummaC-ZS_score  SummaC-Conv_score  \n",
       "0         NaN    NaN        NaN  FacEval        0.608673           0.224066  \n",
       "1         NaN    NaN        NaN  FacEval        0.271927           0.335007  \n",
       "2         NaN    NaN        NaN  FacEval        0.271927           0.335007  \n",
       "3         NaN    NaN        NaN  FacEval        0.090210           0.329921  \n",
       "4         NaN    NaN        NaN  FacEval        0.374329           0.224066  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4233e98-2cb9-4078-8067-c361ca8ade57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transformer['Dialogue'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e021a60-cbde-4736-885d-24af5a2d7791",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "qafact_kwargs = {\"cuda_device\": 0, \"use_lerc_quip\": True, \\\n",
    "        \"verbose\": True, \"generation_batch_size\": 32, \\\n",
    "        \"answering_batch_size\": 32, \"lerc_batch_size\": 8}\n",
    "\n",
    "qafact_model_folder = \"/home/sanjana/factual_evaluation_source_based/experiments/QAFactEval/models\" # path to models downloaded with download_models.sh\n",
    "\n",
    "\n",
    "class Evaluation():\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "\n",
    "    def get_questeval_scores(self, df, source_key, summary_key):\n",
    "        from questeval.questeval_metric import QuestEval\n",
    "        questeval = QuestEval(no_cuda=True)\n",
    "        sources = list(df[source_key].values)\n",
    "        summaries = list(df[summary_key].values)\n",
    "        score = questeval.corpus_questeval(\n",
    "            hypothesis=summaries, \n",
    "            sources=sources,\n",
    "        )\n",
    "        df['QuestEval_score'] = score['ex_level_scores']\n",
    "        return df\n",
    "\n",
    "    def get_qafacteval_scores(self, df, source_key, summary_key):\n",
    "        from qafacteval import QAFactEval\n",
    "        qafacteval = QAFactEval(\n",
    "            lerc_quip_path=f\"{qafact_model_folder}/quip-512-mocha\",\n",
    "            generation_model_path=f\"{qafact_model_folder}/generation/model.tar.gz\",\n",
    "            answering_model_dir=f\"{qafact_model_folder}/answering\",\n",
    "            lerc_model_path=f\"{qafact_model_folder}/lerc/model.tar.gz\",\n",
    "            lerc_pretrained_model_path=f\"{qafact_model_folder}/lerc/pretraining.tar.gz\",\n",
    "            **qafact_kwargs\n",
    "        )\n",
    "        sources = list(df[source_key].values)\n",
    "        summaries = list(df[summary_key].values)\n",
    "        summaries = [[summ] for summ in summaries]\n",
    "        results = qafacteval.score_batch_qafacteval(sources, summaries, return_qa_pairs=True)\n",
    "        scores = [ each[0]['qa-eval']['lerc_quip'] for each in results]\n",
    "        df['QAFactEval_score'] = scores\n",
    "        return df\n",
    "\n",
    "    def get_summac_scores(self, df, source_key, summary_key):\n",
    "        from summac.model_summac import SummaCZS, SummaCConv\n",
    "        model_zs = SummaCZS(granularity=\"sentence\", model_name=\"vitc\", device=\"cuda\") # If you have a GPU: switch to: device=\"cuda\"\n",
    "        model_conv = SummaCConv(models=[\"vitc\"], bins='percentile', granularity=\"sentence\", nli_labels=\"e\", device=\"cuda\", start_file=\"default\", agg=\"mean\")\n",
    "\n",
    "        sources = list(df[source_key].values)\n",
    "        summaries = list(df[summary_key].values)\n",
    "        score_zs = model_zs.score(sources, summaries)\n",
    "        score_conv = model_conv.score(sources, summaries)\n",
    "        df['SummaC-ZS_score']= score_zs['scores']\n",
    "        df['SummaC-Conv_score'] = score_conv['scores']\n",
    "        return df\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db0330b3-52a3-4747-9cf3-c670fa141117",
   "metadata": {},
   "outputs": [],
   "source": [
    "EvalMetrics = Evaluation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2074857-8fc8-4438-88ff-b520721b3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformer = EvalMetrics.get_questeval_scores(df_transformer, 'Dialogue', 'Summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c10a39-e35a-4f13-afd2-bfb624ca0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "905678bb-f7f7-416a-82b8-ea2e336b9c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformer.to_csv('/home/sanjana/factual_evaluation_source_based/datasets/scored/dialogue_aggrefact_scored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b8b08c8-7fe5-4d0b-a6cd-4b49437044a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpus_score': 0.6115375215309217, 'ex_level_scores': [0.569881930994609, 0.6531931120672343]}\n"
     ]
    }
   ],
   "source": [
    "from questeval.questeval_metric import QuestEval\n",
    "questeval = QuestEval(no_cuda=False)\n",
    "\n",
    "source_1 = \"Since 2000, the recipient of the Kate Greenaway medal has also been presented with the Colin Mears award to the value of 35000.\"\n",
    "prediction_1 = \"Since 2000, the winner of the Kate Greenaway medal has also been given to the Colin Mears award of the Kate Greenaway medal.\"\n",
    "references_1 = [\n",
    "    \"Since 2000, the recipient of the Kate Greenaway Medal will also receive the Colin Mears Awad which worth 5000 pounds\",\n",
    "    \"Since 2000, the recipient of the Kate Greenaway Medal has also been given the Colin Mears Award.\"\n",
    "]\n",
    "\n",
    "source_2 = \"He is also a member of another Jungiery boyband 183 Club.\"\n",
    "prediction_2 = \"He also has another Jungiery Boyband 183 club.\"\n",
    "references_2 = [\n",
    "    \"He's also a member of another Jungiery boyband, 183 Club.\", \n",
    "    \"He belonged to the Jungiery boyband 183 Club.\"\n",
    "]\n",
    "\n",
    "score = questeval.corpus_questeval(\n",
    "    hypothesis=[prediction_1, prediction_2], \n",
    "    sources=[source_1, source_2],\n",
    "    list_references=[references_1, references_2]\n",
    ")\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a1bb947-e865-46e3-899e-6135b35a5491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "[Summary 1] SummaCZS Score: 0.583; SummacConv score: 0.536\n",
      "[Summary 2] SummaCZS Score: 0.877; SummacConv score: 0.709\n"
     ]
    }
   ],
   "source": [
    "from summac.model_summac import SummaCZS, SummaCConv\n",
    "\n",
    "model_zs = SummaCZS(granularity=\"sentence\", model_name=\"vitc\", device=\"cuda\") # If you have a GPU: switch to: device=\"cuda\"\n",
    "model_conv = SummaCConv(models=[\"vitc\"], bins='percentile', granularity=\"sentence\", nli_labels=\"e\", device=\"cuda\", start_file=\"default\", agg=\"mean\")\n",
    "\n",
    "document = \"\"\"Scientists are studying Mars to learn about the Red Planet and find landing sites for future missions.\n",
    "One possible site, known as Arcadia Planitia, is covered instrange sinuous features.\n",
    "The shapes could be signs that the area is actually made of glaciers, which are large masses of slow-moving ice.\n",
    "Arcadia Planitia is in Mars' northern lowlands.\"\"\"\n",
    "\n",
    "summary1 = \"There are strange shape patterns on Arcadia Planitia. The shapes could indicate the area might be made of glaciers. This makes Arcadia Planitia ideal for future missions.\"\n",
    "score_zs1 = model_zs.score([document], [summary1])\n",
    "score_conv1 = model_conv.score([document], [summary1])\n",
    "print(\"[Summary 1] SummaCZS Score: %.3f; SummacConv score: %.3f\" % (score_zs1[\"scores\"][0], score_conv1[\"scores\"][0])) # [Summary 1] SummaCZS Score: 0.582; SummacConv score: 0.536\n",
    "\n",
    "summary2 = \"There are strange shape patterns on Arcadia Planitia. The shapes could indicate the area might be made of glaciers.\"\n",
    "score_zs2 = model_zs.score([document, document], [summary2, summary2])\n",
    "score_conv2 = model_conv.score([document], [summary2])\n",
    "print(\"[Summary 2] SummaCZS Score: %.3f; SummacConv score: %.3f\" % (score_zs2[\"scores\"][0], score_conv2[\"scores\"][0])) # [Summary 2] SummaCZS Score: 0.877; SummacConv score: 0.709\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a0d00d-6644-417b-bd0b-da39b5bc7b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 20:07:04.043743: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-30 20:07:05.709060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for _QuestionGenerationModel:\n\tMissing key(s) in state_dict: \"bart.lm_head.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda_device\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_lerc_quip\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \\\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m32\u001b[39m, \\\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswering_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m32\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlerc_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m8\u001b[39m}\n\u001b[1;32m      6\u001b[0m model_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/sanjana/factual_evaluation_source_based/experiments/QAFactEval/models\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# path to models downloaded with download_models.sh\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[43mQAFactEval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlerc_quip_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_folder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/quip-512-mocha\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_folder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/generation/model.tar.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswering_model_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_folder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/answering\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlerc_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_folder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/lerc/model.tar.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlerc_pretrained_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_folder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/lerc/pretraining.tar.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m results \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mscore_batch_qafacteval([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a source document\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a source document\u001b[39m\u001b[38;5;124m\"\u001b[39m], [[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a summary.\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a summary.\u001b[39m\u001b[38;5;124m\"\u001b[39m]], return_qa_pairs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m score \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqa-eval\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlerc_quip\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/factual_evaluation_source_based/experiments/QAFactEval/qafacteval.py:46\u001b[0m, in \u001b[0;36mQAFactEval.__init__\u001b[0;34m(self, lerc_quip_path, use_lerc_quip, lerc_batch_size, cuda_device, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcli\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     spacy\u001b[38;5;241m.\u001b[39mcli\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcuda_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcuda_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_lerc_quip:\n\u001b[1;32m     49\u001b[0m     lerc_quip \u001b[38;5;241m=\u001b[39m LERCQuipScorer(lerc_quip_path\u001b[38;5;241m=\u001b[39mlerc_quip_path, \\\n\u001b[1;32m     50\u001b[0m         cuda_device\u001b[38;5;241m=\u001b[39mcuda_device, batch_size\u001b[38;5;241m=\u001b[39mlerc_batch_size)\n",
      "File \u001b[0;32m/opt/conda/envs/eval_env/lib/python3.8/site-packages/qaeval/metric.py:38\u001b[0m, in \u001b[0;36mQAEval.__init__\u001b[0;34m(self, generation_model_path, answering_model_dir, answer_selection_strategy, cuda_device, generation_batch_size, answering_batch_size, use_lerc, lerc_model_path, lerc_pretrained_model_path, lerc_batch_size, verbose)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     25\u001b[0m     generation_model_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manswer_selector \u001b[38;5;241m=\u001b[39m AnswerSelector(answer_selection_strategy)\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquestion_generator \u001b[38;5;241m=\u001b[39m \u001b[43mQuestionGenerationModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcuda_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcuda_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquestion_answerer \u001b[38;5;241m=\u001b[39m QuestionAnsweringModel(\n\u001b[1;32m     45\u001b[0m         answering_model_dir,\n\u001b[1;32m     46\u001b[0m         cuda_device\u001b[38;5;241m=\u001b[39mcuda_device,\n\u001b[1;32m     47\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39manswering_batch_size,\n\u001b[1;32m     48\u001b[0m         silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m verbose,\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m=\u001b[39m verbose\n",
      "File \u001b[0;32m/opt/conda/envs/eval_env/lib/python3.8/site-packages/qaeval/generation/model.py:31\u001b[0m, in \u001b[0;36mQuestionGenerationModel.__init__\u001b[0;34m(self, model_path, cuda_device, batch_size, silent)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     27\u001b[0m              model_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     28\u001b[0m              cuda_device: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     29\u001b[0m              batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     30\u001b[0m              silent: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m \u001b[43mPredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion_generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcuda_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msilent \u001b[38;5;241m=\u001b[39m silent\n",
      "File \u001b[0;32m/opt/conda/envs/eval_env/lib/python3.8/site-packages/allennlp/predictors/predictor.py:277\u001b[0m, in \u001b[0;36mPredictor.from_path\u001b[0;34m(cls, archive_path, predictor_name, cuda_device, dataset_reader_to_load, frozen, import_plugins)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m import_plugins:\n\u001b[1;32m    275\u001b[0m     plugins\u001b[38;5;241m.\u001b[39mimport_plugins()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Predictor\u001b[38;5;241m.\u001b[39mfrom_archive(\n\u001b[0;32m--> 277\u001b[0m     \u001b[43mload_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchive_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcuda_device\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    278\u001b[0m     predictor_name,\n\u001b[1;32m    279\u001b[0m     dataset_reader_to_load\u001b[38;5;241m=\u001b[39mdataset_reader_to_load,\n\u001b[1;32m    280\u001b[0m     frozen\u001b[38;5;241m=\u001b[39mfrozen,\n\u001b[1;32m    281\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/eval_env/lib/python3.8/site-packages/allennlp/models/archival.py:187\u001b[0m, in \u001b[0;36mload_archive\u001b[0;34m(archive_file, cuda_device, overrides, weights_file)\u001b[0m\n\u001b[1;32m    184\u001b[0m         weights_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(serialization_dir, _DEFAULT_WEIGHTS)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Instantiate model. Use a duplicate of the config, as it will get consumed.\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserialization_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserialization_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcuda_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcuda_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Archive(model\u001b[38;5;241m=\u001b[39mmodel, config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "File \u001b[0;32m/opt/conda/envs/eval_env/lib/python3.8/site-packages/allennlp/models/model.py:367\u001b[0m, in \u001b[0;36mModel.load\u001b[0;34m(cls, config, serialization_dir, weights_file, cuda_device)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_class, \u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# If you're using from_archive to specify your model (e.g., for fine tuning), then you\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;66;03m# can't currently override the behavior of _load; we just use the default Model._load.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;66;03m# If we really need to change this, we would need to implement a recursive\u001b[39;00m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;66;03m# get_model_class method, that recurses whenever it finds a from_archive model type.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m Model\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserialization_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/eval_env/lib/python3.8/site-packages/allennlp/models/model.py:313\u001b[0m, in \u001b[0;36mModel._load\u001b[0;34m(cls, config, serialization_dir, weights_file, cuda_device)\u001b[0m\n\u001b[1;32m    310\u001b[0m model\u001b[38;5;241m.\u001b[39mextend_embedder_vocab()\n\u001b[1;32m    312\u001b[0m model_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(weights_file, map_location\u001b[38;5;241m=\u001b[39mutil\u001b[38;5;241m.\u001b[39mdevice_mapping(cuda_device))\n\u001b[0;32m--> 313\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/envs/eval_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1044\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1040\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1041\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1045\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for _QuestionGenerationModel:\n\tMissing key(s) in state_dict: \"bart.lm_head.weight\". "
     ]
    }
   ],
   "source": [
    "from qafacteval import QAFactEval\n",
    "kwargs = {\"cuda_device\": 0, \"use_lerc_quip\": True, \\\n",
    "        \"verbose\": True, \"generation_batch_size\": 32, \\\n",
    "        \"answering_batch_size\": 32, \"lerc_batch_size\": 8}\n",
    "\n",
    "model_folder = \"/home/sanjana/factual_evaluation_source_based/experiments/QAFactEval/models\" # path to models downloaded with download_models.sh\n",
    "metric = QAFactEval(\n",
    "    lerc_quip_path=f\"{model_folder}/quip-512-mocha\",\n",
    "    generation_model_path=f\"{model_folder}/generation/model.tar.gz\",\n",
    "    answering_model_dir=f\"{model_folder}/answering\",\n",
    "    lerc_model_path=f\"{model_folder}/lerc/model.tar.gz\",\n",
    "    lerc_pretrained_model_path=f\"{model_folder}/lerc/pretraining.tar.gz\",\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "results = metric.score_batch_qafacteval([\"This is a source document\", \"This is a source document\"], [[\"This is a summary.\"], [\"This is a summary.\"]], return_qa_pairs=True)\n",
    "score = results[0][0]['qa-eval']['lerc_quip']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427269f7-961e-40a7-8334-392b27005dca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (eval_env)",
   "language": "python",
   "name": "eval_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
