{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91bb70c-23fc-4db4-87ef-227651b14dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-13 13:14:53.762743: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-13 13:14:54.537382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from GPTModel import GPTInference\n",
    "import spacy\n",
    "import re, string\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0b3fce5-95fb-4276-9859-4cb7c9d5089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def mask_all_errtypes(text):\n",
    "    doc = nlp(text)\n",
    "    origin_tokens = [token for token in doc]\n",
    "    all_masked_results = []\n",
    "    mask_idx = []\n",
    "    mask_tokens = []\n",
    "    for idx, token in enumerate(doc):\n",
    "        append = False\n",
    "        if 'subj' in token.dep_ or 'obj' in token.dep_:\n",
    "            append = True\n",
    "            token_type = 'subjobj'\n",
    "            \n",
    "        elif 'VERB' == token.pos_:\n",
    "            append = True \n",
    "            token_type = 'predicate'\n",
    "\n",
    "        elif 'ADV' == token.pos_:\n",
    "            append = True\n",
    "            token_type = 'circumstance'\n",
    "\n",
    "        elif 'ADP' == token.pos_:\n",
    "            append = True\n",
    "            token_type = 'circumstance'\n",
    "\n",
    "        elif 'PRON' == token.pos_:\n",
    "            append = True\n",
    "            token_type = 'coreference'\n",
    "\n",
    "        \n",
    "\n",
    "        if append:\n",
    "            mask_idx.append(idx)\n",
    "            mask_tokens.append((idx, token.text, token_type))\n",
    "    # print([])\n",
    "    # print(mask_tokens)  \n",
    "\n",
    "    error_types_indices_map = {}\n",
    "    for token_idx, token_text, err_type in mask_tokens:\n",
    "        if err_type not in error_types_indices_map:\n",
    "            error_types_indices_map[err_type] = []\n",
    "        error_types_indices_map[err_type] += [token_idx]\n",
    "        \n",
    "    for err_type, err_indices in error_types_indices_map.items():\n",
    "        for idx in err_indices:\n",
    "            masked_text = [each.text for each in origin_tokens[:idx]] + ['<BLANK>'] + [each.text for each in origin_tokens[idx+1:]]\n",
    "            masked_text = ' '.join(masked_text)\n",
    "            all_masked_results.append((masked_text, origin_tokens[idx].text, err_type))\n",
    "    return all_masked_results\n",
    "\n",
    "\n",
    "def make_masked_sentences(atomic_facts):\n",
    "        masked_atomic_facts_map = {}\n",
    "        for atomic_fact in atomic_facts:\n",
    "            masked_results = mask_all_errtypes(atomic_fact)\n",
    "            for sent, ans, err_type in masked_results:\n",
    "                if err_type not in masked_atomic_facts_map:\n",
    "                    masked_atomic_facts_map[err_type] = []\n",
    "                masked_atomic_facts_map[err_type].append((atomic_fact, sent, ans, err_type))\n",
    "            \n",
    "        return masked_atomic_facts_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2493bf82-ad15-4a69-a0b3-30e1aac055c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_atomic_facts_gpt(gpt_model_atomic, model, text, text_type):\n",
    "    instr = f'Write the {text_type} into indirect speech without introducing any unsupported information or inferences'\n",
    "    prompt = f'{instr}\\nDialogue: {text}'\n",
    "    print(prompt)\n",
    "    gpt_response = gpt_model_atomic.get_chatgpt_response(prompt, model = model)\n",
    "    # print(nltk.sent_tokenize(gpt_response))\n",
    "    return gpt_response\n",
    "    \n",
    "class SyntheticPrompt:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gpt_model_corruptor = GPTInference()\n",
    "        self.gpt_model_sentence_checker = GPTInference()\n",
    "        self.gpt_model_atomic = GPTInference()\n",
    "        self.gpt_model_nli = GPTInference()\n",
    "\n",
    "    def get_atomic_facts(self, gpt_model_type, dlg):\n",
    "        atomic_facts = get_atomic_facts_gpt(self.gpt_model_atomic, gpt_model_type, dlg, 'dialogue')\n",
    "        atomic_facts = nltk.sent_tokenize(atomic_facts)\n",
    "        atomic_facts = [re.sub('[1-9]', '', each) for each in atomic_facts]\n",
    "        atomic_facts = [each.strip(string.punctuation).strip() for each in atomic_facts]\n",
    "        return atomic_facts \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90f92f20-b2b8-4172-8f90-4066b0c1bba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Dialogue</th>\n",
       "      <th>Model</th>\n",
       "      <th>Summary</th>\n",
       "      <th>origin</th>\n",
       "      <th>Annotations</th>\n",
       "      <th>Reference_Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Natacha: hi, i can come and pick you up at the...</td>\n",
       "      <td>gpt3_finetune</td>\n",
       "      <td>Charles will probably arrive at the train stat...</td>\n",
       "      <td>SAMSum</td>\n",
       "      <td>{'Charles will probably arrive at the train st...</td>\n",
       "      <td>Charles has just landed and he will be at RER ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Dialogue  \\\n",
       "0           0  Natacha: hi, i can come and pick you up at the...   \n",
       "\n",
       "           Model                                            Summary  origin  \\\n",
       "0  gpt3_finetune  Charles will probably arrive at the train stat...  SAMSum   \n",
       "\n",
       "                                         Annotations  \\\n",
       "0  {'Charles will probably arrive at the train st...   \n",
       "\n",
       "                                   Reference_Summary  \n",
       "0  Charles has just landed and he will be at RER ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/sanjana/factual_evaluation_source_based/datasets/sota_annotations/dialogue_finegrained_aggrefact.csv')\n",
    "df.head()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75cea94d-af75-42d5-85f3-f1fbcf0d0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[[13]]\n",
    "dialogue = row['Dialogue'].values[0]\n",
    "summary = row['Summary'].values[0]\n",
    "annotations = eval(row['Annotations'].values[0])\n",
    "\n",
    "synthetic_gen = SyntheticPrompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ada6d6f5-6ed4-4f7f-b936-3a0122478791",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_atomic_facts_gpt() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m gpt_model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4-32k-0613\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m atomic_facts \u001b[38;5;241m=\u001b[39m \u001b[43msynthetic_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_atomic_facts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt_model_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdialogue\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 17\u001b[0m, in \u001b[0;36mSyntheticPrompt.get_atomic_facts\u001b[0;34m(self, gpt_model_type, dlg)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_atomic_facts\u001b[39m(\u001b[38;5;28mself\u001b[39m, gpt_model_type, dlg):\n\u001b[0;32m---> 17\u001b[0m     atomic_facts \u001b[38;5;241m=\u001b[39m \u001b[43mget_atomic_facts_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_model_atomic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpt_model_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdlg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdialogue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     atomic_facts \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39msent_tokenize(atomic_facts)\n\u001b[1;32m     19\u001b[0m     atomic_facts \u001b[38;5;241m=\u001b[39m [re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[1-9]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, each) \u001b[38;5;28;01mfor\u001b[39;00m each \u001b[38;5;129;01min\u001b[39;00m atomic_facts]\n",
      "\u001b[0;31mTypeError\u001b[0m: get_atomic_facts_gpt() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "gpt_model_type = 'gpt-4-32k-0613'\n",
    "atomic_facts = synthetic_gen.get_atomic_facts(gpt_model_type, dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8f3c6e5-c740-4c89-b125-7d76c0288f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_atomic_facts_map = make_masked_sentences(atomic_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67242f28-76c8-4a3b-80a8-57950a932be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. Alan found something he was excited about, which he showed in a photo.\\n2. Robert thinks what Alan found has no sugar, no taste, and additional cinnamon flavoring.\\n3. Robert also thinks the item Alan found is nasty.\\n4. Alan enjoys the item he found despite its lack of sugar and taste, and its additional cinnamon flavor.\\n5. According to Robert, Alan has strange tastes.\\n6. Alan then found a perfect company for the item, which he showed in a second photo.\\n7. Robert approves of what Alan found as a companion for the initial item.\\n8. Robert questioned whether whiskey goes well with the cinnamon item.\\n9. Robert dislikes flavored whiskey.\\n10. Alan reassured Robert that the whiskey does taste well with the cinnamon item, provided one puts enough whiskey.\\n11. Rob wished that the old cherry flavor would return.\\n12. Rob dislikes the no-sugar item.\\n13. Alan agrees with Rob's wish for the return of the old cherry flavor.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atomic_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f6257-edd4-482f-a3ff-32182b3105f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
