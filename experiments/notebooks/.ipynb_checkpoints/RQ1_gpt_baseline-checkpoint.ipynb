{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1ea182f-1586-4b2e-a12f-f307059a2d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from GPTModel import GPTInference\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5e5f359-5832-431b-a8b0-b4a2379450b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatGPTBaselines():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.GPTmodel = GPTInference()\n",
    "        self.model_snapshot = 'gpt-4-32k-0613'\n",
    "\n",
    "    def direct_assessment(self, source, summary, source_type  = 'Dialogue'):\n",
    "        instruction = f'''Decide if the following {source_type.lower()} summary is consistent with the corresponding {source_type.lower()}. Note that consistency means all information in the summary is supported by the {source_type.lower()}.'''\n",
    "\n",
    "        prompt = f'{instruction}\\n{source_type}: {source}\\nSummary: {summary}\\nAnswer(yes or no):'\n",
    "        # print(prompt)\n",
    "        response = self.GPTmodel.get_chatgpt_response(prompt, self.model_snapshot)\n",
    "        label = 1 if response.lower().strip() == 'no' else 0\n",
    "        return response, label\n",
    "\n",
    "    def direct_assessment_score(self, source, summary, source_type):\n",
    "        instruction = f'''Score the following summary given the corresponding {source_type.lower()} with respect to consistency on a continuous scale from 0 to 100, where a score of zero means “inconsistency” and score of one hundred means “perfect consistency”. Note that consistency measures whether the facts in the summary are consistent with the facts in the {source_type.lower()}. Consider whether the summary does reproduce all facts accurately and does not make up untrue information.'''\n",
    "        prompt = f'{instruction}\\n{source_type}: {source}\\nSummary: {summary}\\nScore:'\n",
    "        # print(prompt)\n",
    "        response = self.GPTmodel.get_chatgpt_response(prompt, self.model_snapshot)\n",
    "        label = eval(response)\n",
    "        return response, label\n",
    "        \n",
    "    def direct_assessment_stars(self , source, summary, source_type):\n",
    "        instruction = f'''Score the following {source_type.lower()} summarization given the corresponding {source_type.lower()} with respect to consistency with one to five stars, where one star means “inconsistency” and five stars means “perfect consistency”. Note that consistency measures whether the facts in the summary are consistent with the facts in the original article. Consider whether the summary does reproduce all facts accurately and does not make up untrue information.'''\n",
    "        prompt = f'{instruction}\\n{source_type}: {source}\\nSummary: {summary}\\nStars:'\n",
    "        # print(prompt)\n",
    "        response = self.GPTmodel.get_chatgpt_response(prompt, self.model_snapshot)\n",
    "        label = re.findall(r'\\d+', response)\n",
    "        label = eval(label)\n",
    "        return response, label\n",
    "\n",
    "    def direct_assessment_cot(self, source, summary, source_type):\n",
    "        instruction = f'''Decide if the following summary is consistent with the corresponding {source_type.lower()}. Note that consistency means all information in the summary is supported by the article.'''\n",
    "        prompt = f'{instruction}\\n{source_type}: {source}\\nSummary: {summary}\\nExplain your reasoning step by step then answer (yes or no) the question' \n",
    "        # print(prompt)\n",
    "        response = self.GPTmodel.get_chatgpt_response(prompt, self.model_snapshot)\n",
    "        label = 1\n",
    "        if 'summary is consistent' in response:\n",
    "            label = 0\n",
    "        \n",
    "        return response, label\n",
    "        \n",
    "    def get_atomic_facts_gpt(self, text, text_type):\n",
    "        instr = f'Segment the following {text_type.lower()} into atomic facts without introducing any unsupported information'\n",
    "        prompt = f'{instr}\\nDialogue: {text}'\n",
    "        # print(prompt)\n",
    "        gpt_response = self.GPTmodel.get_chatgpt_response(prompt)\n",
    "        return gpt_response\n",
    "\n",
    "    def run_evaluation(self, dlg, summary, type = 'all'):\n",
    "        response_dict = {\n",
    "            \n",
    "        }\n",
    "        \n",
    "        dlg_atomic_facts = self.get_atomic_facts_gpt(dlg, 'Dialogue')\n",
    "        response_dict['atomic_facts'] = dlg_atomic_facts\n",
    "        \n",
    "        response_zs = self.direct_assessment(dlg, summary, source_type = 'Dialogue')\n",
    "        response_zs_afacts = self.direct_assessment(dlg_atomic_facts, summary, source_type = 'Source')\n",
    "        response_dict['text_zs'] = response_zs[0]\n",
    "        response_dict['text_zs_afacts'] = response_zs_afacts[0]\n",
    "        response_dict['label_zs'] = response_zs[1]\n",
    "        response_dict['label_zs_afacts'] = response_zs_afacts[1]\n",
    "        \n",
    "\n",
    "        response_da = self.direct_assessment_score(dlg, summary, source_type = 'Dialogue')\n",
    "        response_da_afacts = self.direct_assessment_score(dlg_atomic_facts, summary, source_type = 'Source')\n",
    "        response_dict['text_da_score'] = response_da[0]\n",
    "        response_dict['text_da_score_afacts'] = response_da_afacts[0]\n",
    "        response_dict['label_da_score'] = response_da[1]\n",
    "        response_dict['label_da_score_afacts'] = response_da_afacts[1]\n",
    "\n",
    "        response_da_stars = self.direct_assessment_stars(dlg, summary, source_type = 'Dialogue')\n",
    "        response_da_stars_afacts = self.direct_assessment_stars(dlg_atomic_facts, summary, source_type = 'Source')\n",
    "        response_dict['text_da_stars'] = response_da_stars[0]\n",
    "        response_dict['text_da_stars_afacts'] = response_da_stars_afacts[0]\n",
    "        response_dict['label_da_stars'] = response_da_stars[1]\n",
    "        response_dict['label_da_stars_afacts'] = response_da_stars_afacts[1]\n",
    "\n",
    "        response_da_cot = self.direct_assessment_cot(dlg, summary, source_type = 'Dialogue')\n",
    "        response_da_cot_afacts = self.direct_assessment_cot(dlg_atomic_facts, summary, source_type = 'Source')\n",
    "        response_dict['text_da_cot'] = response_da_cot[0]\n",
    "        response_dict['text_da_cot_afacts'] = response_da_cot_afacts[0]\n",
    "        response_dict['label_da_cot'] = response_da_cot[1]\n",
    "        response_dict['label_da_cot_afacts'] = response_da_cot_afacts[1]\n",
    "        \n",
    "        return response_dict\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e9640c9-ead8-4231-a5cc-411f497fb585",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_gpt_baselines = ChatGPTBaselines()\n",
    "df_transformer = pd.read_csv('/home/sanjana/factual_evaluation_source_based/datasets/scored/dialogue_aggrefact_scored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45354161-de6b-4af0-a52d-6d9536cada37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cc41309-09e7-4b4a-a661-aea9cbc5c211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                     | 2/5 [00:42<01:03, 21.06s/it]\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/opt/conda/envs/eval_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3508\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[9], line 7\u001b[0m\n    eval_dict = chat_gpt_baselines.run_evaluation(dlg, summary)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[6], line 74\u001b[0m in \u001b[1;35mrun_evaluation\u001b[0m\n    response_da_stars_afacts = self.direct_assessment_stars(dlg_atomic_facts, summary, source_type = 'Source')\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 29\u001b[0;36m in \u001b[0;35mdirect_assessment_stars\u001b[0;36m\n\u001b[0;31m    label = eval(response)\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    4.5 stars\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_response_dict = {}\n",
    "\n",
    "for idx, row in tqdm(df_sample.iterrows(), total=df_sample.shape[0]):\n",
    "    dlg = row['Dialogue']\n",
    "    summary = row['Summary']\n",
    "    \n",
    "    eval_dict = chat_gpt_baselines.run_evaluation(dlg, summary)\n",
    "    for k , v in eval_dict.items():\n",
    "        if k not in df_response_dict:\n",
    "            df_response_dict[k] = []\n",
    "        df_response_dict[k] += [v] \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d1f47-5fcc-46c3-a988-2f014672e2df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (eval_env)",
   "language": "python",
   "name": "eval_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
