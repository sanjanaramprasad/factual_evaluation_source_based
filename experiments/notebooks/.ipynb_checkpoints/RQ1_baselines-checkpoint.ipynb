{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47d6cec-b40f-4fab-9ac9-6572162ef3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from error_config import error_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15dab99d-584f-4fe2-b282-84c8d403e089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformer = pd.read_csv('/home/sanjana/factual_evaluation_source_based/datasets/sota_annotations/dialogue_aggrefact.csv')\n",
    "\n",
    "df_llm = pd.read_csv('/home/sanjana/factual_evaluation_source_based/datasets/llm_annotations/sample/sample_annotations_sanjana.csv')\n",
    "# df_llm = df_llm.sort_values('uuid').drop_duplicates(['docid'], keep='last')\n",
    "\n",
    "len(df_llm)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4233e98-2cb9-4078-8067-c361ca8ade57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>DocID</th>\n",
       "      <th>Dialogue</th>\n",
       "      <th>Model</th>\n",
       "      <th>Summary</th>\n",
       "      <th>w/ Error</th>\n",
       "      <th>CorefE</th>\n",
       "      <th>CorefE_text</th>\n",
       "      <th>CircE</th>\n",
       "      <th>CircE_text</th>\n",
       "      <th>...</th>\n",
       "      <th>GramE_text</th>\n",
       "      <th>PredE</th>\n",
       "      <th>PredE_text</th>\n",
       "      <th>SubjObjE</th>\n",
       "      <th>SubjObjE_text</th>\n",
       "      <th>OtherE</th>\n",
       "      <th>OtherE_text</th>\n",
       "      <th>LinkE</th>\n",
       "      <th>LinkE_text</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13809941</td>\n",
       "      <td>Thelma: i dont have anything to wear\\nLouisa: ...</td>\n",
       "      <td>human_ref</td>\n",
       "      <td>Louisa will lend Thelma her red velvet dress.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FacEval</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     DocID                                           Dialogue  \\\n",
       "0           0  13809941  Thelma: i dont have anything to wear\\nLouisa: ...   \n",
       "\n",
       "       Model                                        Summary  w/ Error  CorefE  \\\n",
       "0  human_ref  Louisa will lend Thelma her red velvet dress.         0     NaN   \n",
       "\n",
       "  CorefE_text  CircE CircE_text  ...  GramE_text PredE  PredE_text SubjObjE  \\\n",
       "0         NaN    NaN        NaN  ...         NaN   NaN         NaN      NaN   \n",
       "\n",
       "   SubjObjE_text OtherE  OtherE_text LinkE  LinkE_text   origin  \n",
       "0            NaN    0.0          NaN   NaN         NaN  FacEval  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformer.head()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e021a60-cbde-4736-885d-24af5a2d7791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from questeval.questeval_metric import QuestEval\n",
    "from qafacteval import QAFactEval\n",
    "\n",
    "qafact_kwargs = {\"cuda_device\": 0, \"use_lerc_quip\": True, \\\n",
    "        \"verbose\": True, \"generation_batch_size\": 32, \\\n",
    "        \"answering_batch_size\": 32, \"lerc_batch_size\": 8}\n",
    "\n",
    "qafact_model_folder = \"/home/sanjana/factual_evaluation_source_based/experiments/QAFactEval/models\" # path to models downloaded with download_models.sh\n",
    "metric = QAFactEval(\n",
    "    lerc_quip_path=f\"{model_folder}/quip-512-mocha\",\n",
    "    generation_model_path=f\"{model_folder}/generation/model.tar.gz\",\n",
    "    answering_model_dir=f\"{model_folder}/answering\",\n",
    "    lerc_model_path=f\"{model_folder}/lerc/model.tar.gz\",\n",
    "    lerc_pretrained_model_path=f\"{model_folder}/lerc/pretraining.tar.gz\",\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "class Evaluation():\n",
    "    def __init__(self):\n",
    "        self.questeval = QuestEval(no_cuda=True)\n",
    "        self.qafacteval = QAFactEval(\n",
    "            lerc_quip_path=f\"{qafact_model_folder}/quip-512-mocha\",\n",
    "            generation_model_path=f\"{qafact_model_folder}/generation/model.tar.gz\",\n",
    "            answering_model_dir=f\"{qafact_model_folder}/answering\",\n",
    "            lerc_model_path=f\"{qafact_model_folder}/lerc/model.tar.gz\",\n",
    "            lerc_pretrained_model_path=f\"{qafact_model_folder}/lerc/pretraining.tar.gz\",\n",
    "            **qafact_kwargs\n",
    "        )\n",
    "\n",
    "    def get_questeval_scores(self, df, source_key, summary_key):\n",
    "        sources = list(df[source_key].value)\n",
    "        summaries = list(df[summary_key].value)\n",
    "        score = self.questeval.corpus_questeval(\n",
    "            hypothesis=summaries, \n",
    "            sources=sources,\n",
    "        )\n",
    "        df['QuestEval_score'] = score['ex_level_scores']\n",
    "\n",
    "    def get_qafacteval_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b8b08c8-7fe5-4d0b-a6cd-4b49437044a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpus_score': 0.6115364581083705, 'ex_level_scores': [0.5698805690757813, 0.6531923471409598]}\n"
     ]
    }
   ],
   "source": [
    "from questeval.questeval_metric import QuestEval\n",
    "questeval = QuestEval(no_cuda=True)\n",
    "\n",
    "source_1 = \"Since 2000, the recipient of the Kate Greenaway medal has also been presented with the Colin Mears award to the value of 35000.\"\n",
    "prediction_1 = \"Since 2000, the winner of the Kate Greenaway medal has also been given to the Colin Mears award of the Kate Greenaway medal.\"\n",
    "references_1 = [\n",
    "    \"Since 2000, the recipient of the Kate Greenaway Medal will also receive the Colin Mears Awad which worth 5000 pounds\",\n",
    "    \"Since 2000, the recipient of the Kate Greenaway Medal has also been given the Colin Mears Award.\"\n",
    "]\n",
    "\n",
    "source_2 = \"He is also a member of another Jungiery boyband 183 Club.\"\n",
    "prediction_2 = \"He also has another Jungiery Boyband 183 club.\"\n",
    "references_2 = [\n",
    "    \"He's also a member of another Jungiery boyband, 183 Club.\", \n",
    "    \"He belonged to the Jungiery boyband 183 Club.\"\n",
    "]\n",
    "\n",
    "score = questeval.corpus_questeval(\n",
    "    hypothesis=[prediction_1, prediction_2], \n",
    "    sources=[source_1, source_2],\n",
    "    list_references=[references_1, references_2]\n",
    ")\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a0d00d-6644-417b-bd0b-da39b5bc7b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-large and are newly initialized: ['final_logits_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /home/sanjana/factual_evaluation_source_based/experiments/QAFactEval/models/quip-512-mocha were not used when initializing RobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /home/sanjana/factual_evaluation_source_based/experiments/QAFactEval/models/quip-512-mocha and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generating questions: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.10it/s]\n",
      "convert squad examples to features: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 298.17it/s]\n",
      "add example index and unique id: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5974.79it/s]\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 27.70it/s]\n",
      "convert squad examples to features: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 260.60it/s]\n",
      "add example index and unique id: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5991.86it/s]\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 29.29it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from qafacteval import QAFactEval\n",
    "kwargs = {\"cuda_device\": 0, \"use_lerc_quip\": True, \\\n",
    "        \"verbose\": True, \"generation_batch_size\": 32, \\\n",
    "        \"answering_batch_size\": 32, \"lerc_batch_size\": 8}\n",
    "\n",
    "model_folder = \"/home/sanjana/factual_evaluation_source_based/experiments/QAFactEval/models\" # path to models downloaded with download_models.sh\n",
    "metric = QAFactEval(\n",
    "    lerc_quip_path=f\"{model_folder}/quip-512-mocha\",\n",
    "    generation_model_path=f\"{model_folder}/generation/model.tar.gz\",\n",
    "    answering_model_dir=f\"{model_folder}/answering\",\n",
    "    lerc_model_path=f\"{model_folder}/lerc/model.tar.gz\",\n",
    "    lerc_pretrained_model_path=f\"{model_folder}/lerc/pretraining.tar.gz\",\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "results = metric.score_batch_qafacteval([\"This is a source document\"], [[\"This is a summary.\"]], return_qa_pairs=True)\n",
    "score = results[0][0]['qa-eval']['lerc_quip']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379da1b5-8370-4722-ac32-ebd526144d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''' Baselines for performance with QuestEval, QAFactEval, SummaC and Prompts'''\n",
    "\n",
    "def get_scores(df):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (eval_env)",
   "language": "python",
   "name": "eval_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
