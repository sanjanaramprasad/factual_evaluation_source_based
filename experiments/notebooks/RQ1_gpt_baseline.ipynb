{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1ea182f-1586-4b2e-a12f-f307059a2d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from GPTModel import GPTInference\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5e5f359-5832-431b-a8b0-b4a2379450b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatGPTBaselines():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.GPTmodel = GPTInference()\n",
    "        self.model_snapshot = 'gpt-4-32k-0613'\n",
    "\n",
    "    def direct_assessment(self, source, summary, source_type  = 'Dialogue'):\n",
    "        instruction = f'''Decide if the following {source_type.lower()} summary is consistent with the corresponding {source_type.lower()}. Note that consistency means all information in the summary is supported by the {source_type.lower()}.'''\n",
    "\n",
    "        prompt = f'{instruction}\\n{source_type}: {source}\\nSummary: {summary}\\nAnswer(yes or no):'\n",
    "        # print(prompt)\n",
    "        response = self.GPTmodel.get_chatgpt_response(prompt, self.model_snapshot)\n",
    "        label = 1 if response.lower().strip() == 'no' else 0\n",
    "        return response, label\n",
    "\n",
    "    def direct_assessment_score(self, source, summary, source_type):\n",
    "        instruction = f'''Score the following summary given the corresponding {source_type.lower()} with respect to consistency on a continuous scale from 0 to 100, where a score of zero means “inconsistency” and score of one hundred means “perfect consistency”. Note that consistency measures whether the facts in the summary are consistent with the facts in the {source_type.lower()}. Consider whether the summary does reproduce all facts accurately and does not make up untrue information.'''\n",
    "        prompt = f'{instruction}\\n{source_type}: {source}\\nSummary: {summary}\\nScore:'\n",
    "        # print(prompt)\n",
    "        response = self.GPTmodel.get_chatgpt_response(prompt, self.model_snapshot)\n",
    "        label = re.findall(r'\\d+', response)[0]\n",
    "        # print(response, label)\n",
    "        label = eval(label)\n",
    "        \n",
    "        return response, label\n",
    "        \n",
    "    def direct_assessment_stars(self , source, summary, source_type):\n",
    "        instruction = f'''Score the following {source_type.lower()} summarization given the corresponding {source_type.lower()} with respect to consistency with one to five stars, where one star means “inconsistency” and five stars means “perfect consistency”. Note that consistency measures whether the facts in the summary are consistent with the facts in the original article. Consider whether the summary does reproduce all facts accurately and does not make up untrue information.'''\n",
    "        prompt = f'{instruction}\\n{source_type}: {source}\\nSummary: {summary}\\nStars:'\n",
    "        # print(prompt)\n",
    "        response = self.GPTmodel.get_chatgpt_response(prompt, self.model_snapshot)\n",
    "        label = re.findall(r'\\d+', response)[0]\n",
    "        # print(response, label)\n",
    "        label = eval(label)\n",
    "        return response, label\n",
    "\n",
    "    def direct_assessment_cot(self, source, summary, source_type):\n",
    "        instruction = f'''Decide if the following summary is consistent with the corresponding {source_type.lower()}. Note that consistency means all information in the summary is supported by the article.'''\n",
    "        prompt = f'{instruction}\\n{source_type}: {source}\\nSummary: {summary}\\nExplain your reasoning step by step then answer (yes or no) the question' \n",
    "        # print(prompt)\n",
    "        response = self.GPTmodel.get_chatgpt_response(prompt, self.model_snapshot)\n",
    "        label = 1\n",
    "        if 'summary is consistent' in response:\n",
    "            label = 0\n",
    "        \n",
    "        return response, label\n",
    "        \n",
    "    def get_atomic_facts_gpt(self, text, text_type):\n",
    "        instr = f'Segment the following {text_type.lower()} into atomic facts without introducing any unsupported information'\n",
    "        prompt = f'{instr}\\nDialogue: {text}'\n",
    "        # print(prompt)\n",
    "        gpt_response = self.GPTmodel.get_chatgpt_response(prompt)\n",
    "        return gpt_response\n",
    "\n",
    "    def run_evaluation(self, dlg, summary, type = 'all'):\n",
    "        response_dict = {\n",
    "            \n",
    "        }\n",
    "        \n",
    "        dlg_atomic_facts = self.get_atomic_facts_gpt(dlg, 'Dialogue')\n",
    "        response_dict['atomic_facts'] = dlg_atomic_facts\n",
    "        \n",
    "        response_zs = self.direct_assessment(dlg, summary, source_type = 'Dialogue')\n",
    "        response_zs_afacts = self.direct_assessment(dlg_atomic_facts, summary, source_type = 'Source')\n",
    "        response_dict['text_zs'] = response_zs[0]\n",
    "        response_dict['text_zs_afacts'] = response_zs_afacts[0]\n",
    "        response_dict['label_zs'] = response_zs[1]\n",
    "        response_dict['label_zs_afacts'] = response_zs_afacts[1]\n",
    "        \n",
    "\n",
    "        response_da = self.direct_assessment_score(dlg, summary, source_type = 'Dialogue')\n",
    "        response_da_afacts = self.direct_assessment_score(dlg_atomic_facts, summary, source_type = 'Source')\n",
    "        response_dict['text_da_score'] = response_da[0]\n",
    "        response_dict['text_da_score_afacts'] = response_da_afacts[0]\n",
    "        response_dict['label_da_score'] = response_da[1]\n",
    "        response_dict['label_da_score_afacts'] = response_da_afacts[1]\n",
    "\n",
    "        response_da_stars = self.direct_assessment_stars(dlg, summary, source_type = 'Dialogue')\n",
    "        response_da_stars_afacts = self.direct_assessment_stars(dlg_atomic_facts, summary, source_type = 'Source')\n",
    "        response_dict['text_da_stars'] = response_da_stars[0]\n",
    "        response_dict['text_da_stars_afacts'] = response_da_stars_afacts[0]\n",
    "        response_dict['label_da_stars'] = response_da_stars[1]\n",
    "        response_dict['label_da_stars_afacts'] = response_da_stars_afacts[1]\n",
    "\n",
    "        response_da_cot = self.direct_assessment_cot(dlg, summary, source_type = 'Dialogue')\n",
    "        response_da_cot_afacts = self.direct_assessment_cot(dlg_atomic_facts, summary, source_type = 'Source')\n",
    "        response_dict['text_da_cot'] = response_da_cot[0]\n",
    "        response_dict['text_da_cot_afacts'] = response_da_cot_afacts[0]\n",
    "        response_dict['label_da_cot'] = response_da_cot[1]\n",
    "        response_dict['label_da_cot_afacts'] = response_da_cot_afacts[1]\n",
    "        \n",
    "        return response_dict\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e9640c9-ead8-4231-a5cc-411f497fb585",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_gpt_baselines = ChatGPTBaselines()\n",
    "df_transformer = pd.read_csv('/home/sanjana/factual_evaluation_source_based/datasets/scored/dialogue_aggrefact_scored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45354161-de6b-4af0-a52d-6d9536cada37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cc41309-09e7-4b4a-a661-aea9cbc5c211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [32:26<00:00, 19.47s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_sample = df_transformer.sample(100)\n",
    "df_response_dict = {}\n",
    "\n",
    "for idx, row in tqdm(df_sample.iterrows(), total=df_sample.shape[0]):\n",
    "    dlg = row['Dialogue']\n",
    "    summary = row['Summary']\n",
    "    \n",
    "    eval_dict = chat_gpt_baselines.run_evaluation(dlg, summary)\n",
    "    for k , v in eval_dict.items():\n",
    "        if k not in df_response_dict:\n",
    "            df_response_dict[k] = []\n",
    "        df_response_dict[k] += [v] \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef3b83-c2e6-474c-bb55-09e032fdfea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k , v in df_response_dict.items():\n",
    "    df_sample[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41275ae2-290b-4d97-afec-0ae348afb506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calculate_auc(y, predictions):\n",
    "    print(roc_auc_score(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d1f47-5fcc-46c3-a988-2f014672e2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "007aa759-a57c-4af1-aa86-e79c27f22da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5663956639566395\n"
     ]
    }
   ],
   "source": [
    "y = df_sample['w/ Error'].values\n",
    "predictions = [each for each in df_sample['label_zs'].values]\n",
    "calculate_auc(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "276b3e8e-eb45-4c73-97d1-14ee7c0632b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5880758807588076\n"
     ]
    }
   ],
   "source": [
    "y = df_sample['w/ Error'].values\n",
    "predictions = [each for each in df_sample['label_zs_afacts'].values]\n",
    "calculate_auc(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ace2a2ba-8b77-49c4-8359-6120d48efa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5948509485094851\n"
     ]
    }
   ],
   "source": [
    "# label_da_cot\n",
    "y = df_sample['w/ Error'].values\n",
    "predictions = [each for each in df_sample['label_da_cot_afacts'].values]\n",
    "calculate_auc(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1b3d5fb-ba9d-4a66-8902-3e902c04e5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6307588075880759\n"
     ]
    }
   ],
   "source": [
    "# label_da_cot\n",
    "y = df_sample['w/ Error'].values\n",
    "predictions = [100 - each for each in df_sample['label_da_score'].values]\n",
    "calculate_auc(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dfd547b8-a2a4-4a4c-a247-b3aea7857e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5924796747967479\n"
     ]
    }
   ],
   "source": [
    "# label_da_cot\n",
    "y = df_sample['w/ Error'].values\n",
    "predictions = [100 - each for each in df_sample['label_da_score_afacts'].values]\n",
    "calculate_auc(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "04ef13ec-03eb-4e7b-ad84-0675ba2ac696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.660230352303523\n"
     ]
    }
   ],
   "source": [
    "# label_da_cot\n",
    "y = df_sample['w/ Error'].values\n",
    "predictions = [5 - each for each in df_sample['label_da_stars'].values]\n",
    "calculate_auc(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "904cb86d-c052-4084-baac-fb27a3bd8a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5975609756097561\n"
     ]
    }
   ],
   "source": [
    "# label_da_cot\n",
    "y = df_sample['w/ Error'].values\n",
    "predictions = [5 - each for each in df_sample['label_da_stars_afacts'].values]\n",
    "calculate_auc(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec952d93-b694-4ec5-b5ff-35137908b2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.2', 'Unnamed: 0.1', 'Unnamed: 0', 'DocID', 'Dialogue',\n",
       "       'Model', 'Summary', 'w/ Error', 'CorefE', 'CorefE_text', 'CircE',\n",
       "       'CircE_text', 'OutE', 'OutE_text', 'GramE', 'GramE_text', 'PredE',\n",
       "       'PredE_text', 'SubjObjE', 'SubjObjE_text', 'OtherE', 'OtherE_text',\n",
       "       'LinkE', 'LinkE_text', 'origin', 'SummaC-ZS_score', 'SummaC-Conv_score',\n",
       "       'QuestEval_score', 'atomic_facts', 'text_zs', 'text_zs_afacts',\n",
       "       'label_zs', 'label_zs_afacts', 'text_da_score', 'text_da_score_afacts',\n",
       "       'label_da_score', 'label_da_score_afacts', 'text_da_stars',\n",
       "       'text_da_stars_afacts', 'label_da_stars', 'label_da_stars_afacts',\n",
       "       'text_da_cot', 'text_da_cot_afacts', 'label_da_cot',\n",
       "       'label_da_cot_afacts'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18d02ad3-27f5-4b37-9fb5-db01a4b67b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216      80\n",
       "1068    100\n",
       "1511     70\n",
       "829     100\n",
       "998      95\n",
       "       ... \n",
       "147     100\n",
       "740     100\n",
       "1438    100\n",
       "604     100\n",
       "1171     70\n",
       "Name: label_da_score, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_sample['label_da_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc7c08b-c2dc-4e0e-ae5e-28b039e9ac53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (eval_env)",
   "language": "python",
   "name": "eval_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
